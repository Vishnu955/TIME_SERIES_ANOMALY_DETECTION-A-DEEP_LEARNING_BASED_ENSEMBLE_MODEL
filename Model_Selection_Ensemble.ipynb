{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "collapsed_sections": [
        "AZ_7hUaI19xl",
        "6SwzaUqQ3pze"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Required libraries**"
      ],
      "metadata": {
        "id": "OBjAZiYE9Z7a"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "40SZENMx8uOK",
        "outputId": "df0d0dba-0b64-4ce7-a727-a09ab6f14e30"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting tensorflow-gpu\n",
            "  Using cached tensorflow-gpu-2.12.0.tar.gz (2.6 kB)\n",
            "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
            "  \n",
            "  \u001b[31m×\u001b[0m \u001b[32mpython setup.py egg_info\u001b[0m did not run successfully.\n",
            "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
            "  \u001b[31m╰─>\u001b[0m See above for output.\n",
            "  \n",
            "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25herror\n",
            "\u001b[1;31merror\u001b[0m: \u001b[1mmetadata-generation-failed\u001b[0m\n",
            "\n",
            "\u001b[31m×\u001b[0m Encountered error while generating package metadata.\n",
            "\u001b[31m╰─>\u001b[0m See above for output.\n",
            "\n",
            "\u001b[1;35mnote\u001b[0m: This is an issue with the package mentioned above, not pip.\n",
            "\u001b[1;36mhint\u001b[0m: See above for details.\n"
          ]
        }
      ],
      "source": [
        "#installling required packages\n",
        "!pip install --no-cache-dir -q torch-scatter -f https://data.pyg.org/whl/torch-1.11.0+cu113.html\n",
        "!pip install --no-cache-dir -q torch-sparse -f https://data.pyg.org/whl/torch-1.11.0+cu113.html\n",
        "!pip install --no-cache-dir -q git+https://github.com/pyg-team/pytorch_geometric.git\n",
        "!pip install tensorflow-gpu"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Affn_uQo-v4L"
      },
      "outputs": [],
      "source": [
        "#importing required libraries\n",
        "from scipy.stats import rankdata, iqr, trim_mean\n",
        "from scipy.stats import rankdata\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import RobustScaler\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error,mean_absolute_percentage_error\n",
        "from sklearn.metrics import precision_score, recall_score, roc_auc_score, f1_score\n",
        "\n",
        "import time\n",
        "import math\n",
        "import csv\n",
        "import time\n",
        "import logging\n",
        "import random\n",
        "from traceback import print_tb\n",
        "import os\n",
        "from datetime import datetime as dt\n",
        "import more_itertools as mit\n",
        "import argparse\n",
        "import json\n",
        "import sys\n",
        "import itertools as it\n",
        "\n",
        "import numpy as np\n",
        "from numpy import percentile\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader, random_split, Subset\n",
        "import torch.nn as nn\n",
        "from torch_geometric.nn import GCNConv, GATConv, EdgeConv\n",
        "import torch.nn.functional as F\n",
        "from torch.nn import Parameter, Linear, Sequential, BatchNorm1d, ReLU\n",
        "import torch.nn.functional as F\n",
        "from torch_geometric.nn.conv import MessagePassing\n",
        "from torch_geometric.utils import remove_self_loops, add_self_loops, softmax\n",
        "from torch.utils.data import SequentialSampler\n",
        "from torch_geometric.nn.inits import glorot, zeros\n",
        "\n",
        "from prophet import Prophet\n",
        "import plotly.graph_objects as go\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential, load_model\n",
        "from tensorflow.keras.layers import LSTM, Dropout, Dense, Activation\n",
        "from tensorflow.keras.callbacks import History, EarlyStopping\n",
        "from joblib import Parallel, delayed\n",
        "\n",
        "from keras.models import Sequential\n",
        "from keras.callbacks import History, EarlyStopping, Callback\n",
        "from keras.layers import LSTM, Dense, Activation, Dropout\n",
        "\n",
        "logger = logging.getLogger('lstmndt')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **LSTM-NDT**"
      ],
      "metadata": {
        "id": "AZ_7hUaI19xl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class Config:\n",
        "    #to load parameters from config_dict into the global object\n",
        "\n",
        "    def __init__(self, config_dict):\n",
        "        self.dictionary = config_dict\n",
        "\n",
        "        for k, v in self.dictionary.items():\n",
        "            setattr(self, k, v)\n",
        "\n",
        "\n",
        "\n",
        "def make_dirs():\n",
        "    #to create directories for storing data if they don't already exist\n",
        "\n",
        "    config = Config(config_lstmndt)\n",
        "\n",
        "    paths = ['models', 'logs']\n",
        "\n",
        "    for p in paths:\n",
        "        if not os.path.isdir(p):\n",
        "            os.mkdir(p)\n",
        "\n",
        "\n",
        "def setup_logging():\n",
        "    #Configuring the logging object to track training, and evaluation.\n",
        "\n",
        "    logger = logging.getLogger('lstmndt')\n",
        "    logger.setLevel(logging.INFO)\n",
        "\n",
        "    stdout = logging.StreamHandler(sys.stdout)\n",
        "    stdout.setLevel(logging.INFO)\n",
        "    logger.addHandler(stdout)\n",
        "\n",
        "    return logger\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "YwZVwxpk2Kdo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Channel:\n",
        "    def __init__(self, config, chan_id):\n",
        "\n",
        "        # To Load and reshape predicted values and actual and actual values for a channel\n",
        "\n",
        "        self.id = chan_id\n",
        "        self.config = config\n",
        "        self.X_train = None #training inputs\n",
        "        self.y_train = None #actual channel training values\n",
        "        self.X_test = None #test inputs\n",
        "        self.y_test = None #actual channel test values\n",
        "        self.y_hat = None #forecast values\n",
        "        self.train = None #train data loaded from .csv file\n",
        "        self.test = None # test data loaded from .csv file\n",
        "\n",
        "    def shape_data(self, arr, train=True):\n",
        "        #Shape raw input streams for ingestion into LSTM. config.l_s specifies\n",
        "\n",
        "        data = []\n",
        "        for i in range(len(arr) - self.config.l_s - self.config.n_predictions):\n",
        "            data.append(arr[i:i + self.config.l_s + self.config.n_predictions])\n",
        "        data = np.array(data)\n",
        "\n",
        "        assert len(data.shape) == 3\n",
        "\n",
        "        if train:\n",
        "            np.random.shuffle(data)\n",
        "            self.X_train = data[:, :-self.config.n_predictions, :]\n",
        "            self.y_train = data[:, -self.config.n_predictions:, 0]\n",
        "        else:\n",
        "            self.X_test = data[:, :-self.config.n_predictions, :]\n",
        "            self.y_test = data[:, -self.config.n_predictions:, 0]\n",
        "\n",
        "\n",
        "    def load_data(self, train_path=None, test_path=None):\n",
        "\n",
        "      #Load train and test data from local CSV files.\n",
        "\n",
        "      if train_path and test_path:\n",
        "        train_data = pd.read_csv(train_path)\n",
        "        test_data = pd.read_csv(test_path)\n",
        "\n",
        "      elif train_path:\n",
        "        data = pd.read_csv(train_path)\n",
        "        train_data = data.sample(frac=0.8, random_state=42)\n",
        "        test_data = data.drop(train_data.index)\n",
        "\n",
        "      else:\n",
        "        raise ValueError(\"Either train_path or both train_path and test_path must be provided.\")\n",
        "\n",
        "      # Extract the values of the channel for normalization\n",
        "      train_values = train_data[self.id].values.reshape(-1, 1)\n",
        "      test_values = test_data[self.id].values.reshape(-1, 1)\n",
        "\n",
        "      # Perform robust scaling using RobustScaler\n",
        "      scaler = RobustScaler()\n",
        "      train_values_scaled = scaler.fit_transform(train_values)\n",
        "      test_values_scaled = scaler.transform(test_values)\n",
        "\n",
        "      # Assign the scaled values back to the train and test data\n",
        "      train_data[self.id] = train_values_scaled\n",
        "      test_data[self.id] = test_values_scaled\n",
        "\n",
        "      self.train = train_data[self.id].values.reshape(-1, 1)\n",
        "      self.test = test_data[self.id].values.reshape(-1, 1)\n",
        "\n",
        "      self.shape_data(self.train)\n",
        "      self.shape_data(self.test, train=False)\n"
      ],
      "metadata": {
        "id": "7G8PBWVS10IV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Errors:\n",
        "    def __init__(self, channel, config, run_id):\n",
        "\n",
        "        #Batch processing of errors between actual and predicted values for a channel.\n",
        "\n",
        "        self.config = config\n",
        "        self.window_size = self.config.window_size # number of trailing batches to use in error calculation\n",
        "        self.n_windows = int((channel.y_test.shape[0] -\n",
        "                              (self.config.batch_size * self.window_size))\n",
        "                             / self.config.batch_size)\n",
        "\n",
        "        self.i_anom = np.array([])  #indices of anomalies in channel test values\n",
        "        self.E_seq = [] #array of (start, end) indices for each continuous anomaly sequence in test values\n",
        "        self.anom_scores = [] #score indicating relative severity of each anomaly sequence in E_seq\n",
        "\n",
        "        # raw prediction error\n",
        "        self.e = [abs(y_h - y_t[0]) for y_h, y_t in\n",
        "                  zip(channel.y_hat, channel.y_test)]  #errors in prediction (predicted - actual)\n",
        "\n",
        "        smoothing_window = int(self.config.batch_size * self.config.window_size\n",
        "                               * self.config.smoothing_perc)\n",
        "        if not len(channel.y_hat) == len(channel.y_test):\n",
        "            raise ValueError('len(y_hat) != len(y_test): {}, {}'\n",
        "                             .format(len(channel.y_hat), len(channel.y_test)))\n",
        "\n",
        "        # smoothed prediction error\n",
        "        self.e_s = pd.DataFrame(self.e).ewm(span=smoothing_window) \\\n",
        "            .mean().values.flatten()\n",
        "\n",
        "        # for values at beginning < sequence length, just use avg\n",
        "        self.e_s[:self.config.l_s] = \\\n",
        "            [np.mean(self.e_s[:self.config.l_s * 2])] * self.config.l_s\n",
        "\n",
        "        self.normalized = np.mean(self.e / np.ptp(channel.y_test))\n",
        "        logger.info(\"normalized prediction error: {0:.2f}\"\n",
        "                    .format(self.normalized))\n",
        "\n",
        "    def adjust_window_size(self, channel):\n",
        "\n",
        "       # to decrease the historical error window size (h) if number of test values is limited\n",
        "\n",
        "        while self.n_windows < 0:\n",
        "            self.window_size -= 1\n",
        "            self.n_windows = int((channel.y_test.shape[0]\n",
        "                                 - (self.config.batch_size * self.window_size))\n",
        "                                 / self.config.batch_size)\n",
        "            if self.window_size == 1 and self.n_windows < 0:\n",
        "                raise ValueError('Batch_size ({}) larger than y_test (len={}). '\n",
        "                                 'Adjust in config.yaml.'\n",
        "                                 .format(self.config.batch_size,\n",
        "                                         channel.y_test.shape[0]))\n",
        "\n",
        "    def merge_scores(self):\n",
        "        '''\n",
        "        If anomalous sequences from subsequent batches are adjacent they will\n",
        "        automatically be combined. This combines the scores for these initial\n",
        "        adjacent sequences (scores are calculated as each batch is processed)\n",
        "        where applicable.\n",
        "        '''\n",
        "\n",
        "        merged_scores = []\n",
        "        score_end_indices = []\n",
        "\n",
        "        for i, score in enumerate(self.anom_scores):\n",
        "            if not score['start_idx'] - 1 in score_end_indices:\n",
        "                merged_scores.append(score['score'])\n",
        "                score_end_indices.append(score['end_idx'])\n",
        "\n",
        "    def process_batches(self, channel):\n",
        "\n",
        "        #to loops through batches of values for a channel.\n",
        "\n",
        "\n",
        "        self.adjust_window_size(channel)\n",
        "\n",
        "        for i in range(0, self.n_windows + 1):\n",
        "            prior_idx = i * self.config.batch_size\n",
        "            idx = (self.config.window_size * self.config.batch_size) \\\n",
        "                  + (i * self.config.batch_size)\n",
        "            if i == self.n_windows:\n",
        "                idx = channel.y_test.shape[0]\n",
        "\n",
        "            window = ErrorWindow(channel, self.config, prior_idx, idx, self, i)\n",
        "\n",
        "            window.find_epsilon()\n",
        "            window.find_epsilon(inverse=True)\n",
        "\n",
        "            window.compare_to_epsilon(self)\n",
        "            window.compare_to_epsilon(self, inverse=True)\n",
        "\n",
        "            if len(window.i_anom) == 0 and len(window.i_anom_inv) == 0:\n",
        "                continue\n",
        "\n",
        "            window.prune_anoms()\n",
        "            window.prune_anoms(inverse=True)\n",
        "\n",
        "            if len(window.i_anom) == 0 and len(window.i_anom_inv) == 0:\n",
        "                continue\n",
        "\n",
        "            window.i_anom = np.sort(np.unique(\n",
        "                np.append(window.i_anom, window.i_anom_inv))).astype('int')\n",
        "            window.score_anomalies(prior_idx)\n",
        "\n",
        "            # update indices to reflect true indices in full set of values\n",
        "            self.i_anom = np.append(self.i_anom, window.i_anom + prior_idx)\n",
        "            self.anom_scores = self.anom_scores + window.anom_scores\n",
        "\n",
        "        if len(self.i_anom) > 0:\n",
        "            # group anomalous indices into continuous sequences\n",
        "            groups = [list(group) for group in\n",
        "                      mit.consecutive_groups(self.i_anom)]\n",
        "            self.E_seq = [(int(g[0]), int(g[-1])) for g in groups\n",
        "                          if not g[0] == g[-1]]\n",
        "\n",
        "            '''additional shift is applied to indices so that they represent the\n",
        "            position in the original data array, obtained from the .csv files,\n",
        "            and not the position on y_test'''\n",
        "            self.E_seq = [(e_seq[0] + self.config.l_s,\n",
        "                           e_seq[1] + self.config.l_s) for e_seq in self.E_seq]\n",
        "\n",
        "            self.merge_scores()\n",
        "\n"
      ],
      "metadata": {
        "id": "ZXPBmWiS29Zs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ErrorWindow:\n",
        "    '''\n",
        "    Data and calculations for a specific window of prediction errors.\n",
        "    Includes finding thresholds, pruning, and scoring anomalous sequences\n",
        "    for errors and inverted errors (flipped around mean) - significant drops\n",
        "    in values can also be anomalous.'''\n",
        "\n",
        "\n",
        "\n",
        "    def __init__(self, channel, config, start_idx, end_idx, errors, window_num):\n",
        "        self.i_anom = np.array([])\n",
        "        self.E_seq = np.array([])\n",
        "        self.non_anom_max = -1000000   #highest smoothed error value below epsilon\n",
        "        self.i_anom_inv = np.array([])\n",
        "        self.E_seq_inv = np.array([])\n",
        "        self.non_anom_max_inv = -1000000  #highest smoothed error value below epsilon_inv\n",
        "\n",
        "        self.config = config\n",
        "        self.anom_scores = []\n",
        "\n",
        "        self.window_num = window_num\n",
        "\n",
        "        self.sd_lim = 12.0  #default number of standard deviations to use for threshold if no winner or too many anomalous ranges when scoring candidate thresholds\n",
        "        self.sd_threshold = self.sd_lim  #number of standard deviations for calculation of best anomaly threshold\n",
        "        self.sd_threshold_inv = self.sd_lim #for inverted channel values\n",
        "\n",
        "        self.e_s = errors.e_s[start_idx:end_idx]\n",
        "        self.mean_e_s = np.mean(self.e_s) #mean of smoothed errors\n",
        "        self.sd_e_s = np.std(self.e_s) #sd of smoothed errors\n",
        "        self.e_s_inv = np.array([self.mean_e_s + (self.mean_e_s - e)\n",
        "                                 for e in self.e_s])\n",
        "\n",
        "        self.epsilon = self.mean_e_s + self.sd_lim * self.sd_e_s  #threshold for e_s above which an error is considered anomalous\n",
        "        self.epsilon_inv = self.mean_e_s + self.sd_lim * self.sd_e_s\n",
        "\n",
        "        self.y_test = channel.y_test[start_idx:end_idx]  #actual values\n",
        "        self.sd_values = np.std(self.y_test) #st dev of y_test\n",
        "\n",
        "        self.perc_high, self.perc_low = np.percentile(self.y_test, [95, 5])  #the 95th percentile of y_test values, the 5th percentile of y_test values\n",
        "        self.inter_range = self.perc_high - self.perc_low #the range between perc_high - perc_low\n",
        "\n",
        "        # ignore initial error values until enough history for processing\n",
        "        self.num_to_ignore = self.config.l_s * 2\n",
        "        # if y_test is small, ignore fewer\n",
        "        if len(channel.y_test) < 2500:\n",
        "            self.num_to_ignore = self.config.l_s\n",
        "        if len(channel.y_test) < 1800:\n",
        "            self.num_to_ignore = 0\n",
        "\n",
        "    def find_epsilon(self, inverse=False):\n",
        "        '''\n",
        "        Find the anomaly threshold that maximizes function representing tradeoff\n",
        "        between the number of anomalies and anomalous ranges and the reduction\n",
        "        in mean and st dev if anomalous points are removed from errors '''\n",
        "\n",
        "        e_s = self.e_s if not inverse else self.e_s_inv\n",
        "\n",
        "        max_score = -10000000\n",
        "\n",
        "        for z in np.arange(2.5, self.sd_lim, 0.5):\n",
        "            epsilon = self.mean_e_s + (self.sd_e_s * z)\n",
        "\n",
        "            pruned_e_s = e_s[e_s < epsilon]\n",
        "\n",
        "            i_anom = np.argwhere(e_s >= epsilon).reshape(-1,)\n",
        "            buffer = np.arange(1, self.config.error_buffer)\n",
        "            i_anom = np.sort(np.concatenate((i_anom,\n",
        "                                            np.array([i+buffer for i in i_anom])\n",
        "                                             .flatten(),\n",
        "                                            np.array([i-buffer for i in i_anom])\n",
        "                                             .flatten())))\n",
        "            i_anom = i_anom[(i_anom < len(e_s)) & (i_anom >= 0)]\n",
        "            i_anom = np.sort(np.unique(i_anom))\n",
        "\n",
        "            if len(i_anom) > 0:\n",
        "                # group anomalous indices into continuous sequences\n",
        "                groups = [list(group) for group\n",
        "                          in mit.consecutive_groups(i_anom)]\n",
        "                E_seq = [(g[0], g[-1]) for g in groups if not g[0] == g[-1]]\n",
        "\n",
        "                mean_perc_decrease = (self.mean_e_s - np.mean(pruned_e_s)) \\\n",
        "                                     / self.mean_e_s\n",
        "                sd_perc_decrease = (self.sd_e_s - np.std(pruned_e_s)) \\\n",
        "                                   / self.sd_e_s\n",
        "                score = (mean_perc_decrease + sd_perc_decrease) \\\n",
        "                        / (len(E_seq) ** 2 + len(i_anom))\n",
        "\n",
        "                # sanity checks / guardrails\n",
        "                if score >= max_score and len(E_seq) <= 5 and \\\n",
        "                        len(i_anom) < (len(e_s) * 0.5):\n",
        "                    max_score = score\n",
        "                    if not inverse:\n",
        "                        self.sd_threshold = z\n",
        "                        self.epsilon = self.mean_e_s + z * self.sd_e_s\n",
        "                    else:\n",
        "                        self.sd_threshold_inv = z\n",
        "                        self.epsilon_inv = self.mean_e_s + z * self.sd_e_s\n",
        "\n",
        "    def compare_to_epsilon(self, errors_all, inverse=False):\n",
        "        '''\n",
        "        Compare smoothed error values to epsilon (error threshold) and group\n",
        "        consecutive errors together into sequences '''\n",
        "\n",
        "        e_s = self.e_s if not inverse else self.e_s_inv\n",
        "        epsilon = self.epsilon if not inverse else self.epsilon_inv\n",
        "\n",
        "        # Check if scale of errors compared to values too small\n",
        "        if not (self.sd_e_s > (.05 * self.sd_values) or max(self.e_s)\n",
        "                > (.05 * self.inter_range)) or not max(self.e_s) > 0.05:\n",
        "            return\n",
        "\n",
        "        i_anom = np.argwhere((e_s >= epsilon) &\n",
        "                             (e_s > 0.05 * self.inter_range)).reshape(-1,)\n",
        "\n",
        "        if len(i_anom) == 0:\n",
        "            return\n",
        "        buffer = np.arange(1, self.config.error_buffer+1)\n",
        "        i_anom = np.sort(np.concatenate((i_anom,\n",
        "                                         np.array([i + buffer for i in i_anom])\n",
        "                                         .flatten(),\n",
        "                                         np.array([i - buffer for i in i_anom])\n",
        "                                         .flatten())))\n",
        "        i_anom = i_anom[(i_anom < len(e_s)) & (i_anom >= 0)]\n",
        "\n",
        "        # if it is first window, ignore initial errors\n",
        "        if self.window_num == 0:\n",
        "            i_anom = i_anom[i_anom >= self.num_to_ignore]\n",
        "        else:\n",
        "            i_anom = i_anom[i_anom >= len(e_s) - self.config.batch_size]\n",
        "\n",
        "        i_anom = np.sort(np.unique(i_anom))\n",
        "\n",
        "        # capture max of non-anomalous values below the threshold\n",
        "        batch_position = self.window_num * self.config.batch_size\n",
        "        window_indices = np.arange(0, len(e_s)) + batch_position\n",
        "        adj_i_anom = i_anom + batch_position\n",
        "        window_indices = np.setdiff1d(window_indices,\n",
        "                                      np.append(errors_all.i_anom, adj_i_anom))\n",
        "        candidate_indices = np.unique(window_indices - batch_position)\n",
        "        non_anom_max = np.max(np.take(e_s, candidate_indices))\n",
        "\n",
        "        # group anomalous indices into continuous sequences\n",
        "        groups = [list(group) for group in mit.consecutive_groups(i_anom)]\n",
        "        E_seq = [(g[0], g[-1]) for g in groups if not g[0] == g[-1]]\n",
        "\n",
        "        if inverse:\n",
        "            self.i_anom_inv = i_anom\n",
        "            self.E_seq_inv = E_seq\n",
        "            self.non_anom_max_inv = non_anom_max\n",
        "        else:\n",
        "            self.i_anom = i_anom\n",
        "            self.E_seq = E_seq\n",
        "            self.non_anom_max = non_anom_max\n",
        "\n",
        "    def prune_anoms(self, inverse=False):\n",
        "        '''\n",
        "        Remove anomalies that don't meet minimum separation from the next\n",
        "        closest anomaly or error value'''\n",
        "\n",
        "        E_seq = self.E_seq if not inverse else self.E_seq_inv\n",
        "        e_s = self.e_s if not inverse else self.e_s_inv\n",
        "        non_anom_max = self.non_anom_max if not inverse \\\n",
        "            else self.non_anom_max_inv\n",
        "\n",
        "        if len(E_seq) == 0:\n",
        "            return\n",
        "\n",
        "        E_seq_max = np.array([max(e_s[e[0]:e[1]+1]) for e in E_seq])\n",
        "        E_seq_max_sorted = np.sort(E_seq_max)[::-1]\n",
        "        E_seq_max_sorted = np.append(E_seq_max_sorted, [non_anom_max])\n",
        "\n",
        "        i_to_remove = np.array([])\n",
        "        for i in range(0, len(E_seq_max_sorted)-1):\n",
        "            if (E_seq_max_sorted[i] - E_seq_max_sorted[i+1]) \\\n",
        "                    / E_seq_max_sorted[i] < self.config.p:\n",
        "                i_to_remove = np.append(i_to_remove, np.argwhere(\n",
        "                    E_seq_max == E_seq_max_sorted[i]))\n",
        "            else:\n",
        "                i_to_remove = np.array([])\n",
        "        i_to_remove[::-1].sort()\n",
        "        i_to_remove = i_to_remove.astype(int)\n",
        "        if len(i_to_remove) > 0:\n",
        "            E_seq = np.delete(E_seq, i_to_remove, axis=0)\n",
        "\n",
        "        if len(E_seq) == 0 and inverse:\n",
        "            self.i_anom_inv = np.array([])\n",
        "            return\n",
        "        elif len(E_seq) == 0 and not inverse:\n",
        "            self.i_anom = np.array([])\n",
        "            return\n",
        "\n",
        "        indices_to_keep = np.concatenate([range(e_seq[0], e_seq[-1]+1)\n",
        "                                          for e_seq in E_seq])\n",
        "\n",
        "        if not inverse:\n",
        "            mask = np.isin(self.i_anom, indices_to_keep)\n",
        "            self.i_anom = self.i_anom[mask]\n",
        "        else:\n",
        "            mask_inv = np.isin(self.i_anom_inv, indices_to_keep)\n",
        "            self.i_anom_inv = self.i_anom_inv[mask_inv]\n",
        "\n",
        "    def score_anomalies(self, prior_idx):\n",
        "        '''\n",
        "        Calculate anomaly scores based on max distance from epsilon\n",
        "        for each anomalous sequence '''\n",
        "\n",
        "        groups = [list(group) for group in mit.consecutive_groups(self.i_anom)]\n",
        "\n",
        "        for e_seq in groups:\n",
        "\n",
        "            score_dict = {\n",
        "                \"start_idx\": e_seq[0] + prior_idx,\n",
        "                \"end_idx\": e_seq[-1] + prior_idx,\n",
        "                \"score\": 0\n",
        "            }\n",
        "\n",
        "            score = max([abs(self.e_s[i] - self.epsilon)\n",
        "                         / (self.mean_e_s + self.sd_e_s) for i in\n",
        "                         range(e_seq[0], e_seq[-1] + 1)])\n",
        "            inv_score = max([abs(self.e_s_inv[i] - self.epsilon_inv)\n",
        "                             / (self.mean_e_s + self.sd_e_s) for i in\n",
        "                             range(e_seq[0], e_seq[-1] + 1)])\n",
        "\n",
        "            # the max score indicates whether anomaly was from regular\n",
        "            # or inverted errors\n",
        "            score_dict['score'] = max([score, inv_score])\n",
        "            self.anom_scores.append(score_dict)\n",
        "\n"
      ],
      "metadata": {
        "id": "mLZm1VXc4VxP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
        "\n",
        "\n",
        "class Model:\n",
        "    def __init__(self, config, run_id, channel):\n",
        "        '''\n",
        "        Loads/trains LSTM model and predicts future values for a given channel '''\n",
        "\n",
        "        self.config = config\n",
        "        self.chan_id = channel.id\n",
        "        self.run_id = run_id\n",
        "        self.y_hat = np.array([])\n",
        "        self.model = None\n",
        "\n",
        "        if not self.config.train:\n",
        "            try:\n",
        "                self.load(channel)\n",
        "            except FileNotFoundError:\n",
        "                path = os.path.join('models',self.chan_id + '.h5')\n",
        "                logger.warning('Training new model, couldn\\'t find existing '\n",
        "                               'model at {}'.format(path))\n",
        "                self.train_new(channel)\n",
        "                self.save()\n",
        "        else:\n",
        "            self.train_new(channel)\n",
        "            self.save()\n",
        "\n",
        "    def load(self,channel):\n",
        "\n",
        "        # to load model for channel.\n",
        "\n",
        "\n",
        "        logger.info('Loading pre-trained model')\n",
        "        self.model = load_model(os.path.join('models', self.chan_id + '.h5'))\n",
        "        train_metrics = self.model.evaluate(channel.X_train, channel.y_train, verbose=0)\n",
        "        self.mae = train_metrics[1]\n",
        "        self.mse = train_metrics[2]\n",
        "        self.mape = train_metrics[3]\n",
        "\n",
        "    def train_new(self, channel):\n",
        "\n",
        "        #Train LSTM model according to specifications in config\n",
        "\n",
        "        cbs = [History(), EarlyStopping(monitor='val_loss',\n",
        "                                        patience=self.config.patience,\n",
        "                                        min_delta=self.config.min_delta,\n",
        "                                        verbose=0)]\n",
        "\n",
        "        with tf.device('/GPU:0'):\n",
        "            self.model = Sequential()\n",
        "\n",
        "            self.model.add(LSTM(\n",
        "                self.config.layers[0],\n",
        "                input_shape=(None, channel.X_train.shape[2]),\n",
        "                return_sequences=True))\n",
        "            self.model.add(Dropout(self.config.dropout))\n",
        "\n",
        "            self.model.add(LSTM(\n",
        "                self.config.layers[1],\n",
        "                return_sequences=False))\n",
        "            self.model.add(Dropout(self.config.dropout))\n",
        "\n",
        "            self.model.add(Dense(\n",
        "                self.config.n_predictions))\n",
        "            self.model.add(Activation('linear'))\n",
        "\n",
        "            self.model.compile(loss=self.config.loss_metric,\n",
        "                               optimizer=self.config.optimizer, metrics=['mae','mse','mape'])\n",
        "\n",
        "\n",
        "            history=self.model.fit(channel.X_train,\n",
        "                           channel.y_train,\n",
        "                           batch_size=self.config.lstm_batch_size,\n",
        "                           epochs=self.config.epochs,\n",
        "                           validation_split=self.config.validation_split,\n",
        "                           callbacks=cbs,\n",
        "                           verbose=True)\n",
        "            self.mae = history.history['mae'][-1]\n",
        "            self.mse = history.history['mse'][-1]\n",
        "            self.mape = history.history['mape'][-1]\n",
        "\n",
        "    def save(self):\n",
        "\n",
        "        #Save trained model.\n",
        "        self.model.save(os.path.join('models',\n",
        "                                     '{}.h5'.format(self.chan_id)))\n",
        "\n",
        "    def aggregate_predictions(self, y_hat_batch, method='first'):\n",
        "        '''\n",
        "         to aggregate predictions for each timestep. When predicting n steps\n",
        "        ahead where n > 1, will end up with multiple predictions for a\n",
        "        timestep'''\n",
        "\n",
        "        agg_y_hat_batch = np.array([])\n",
        "\n",
        "        for t in range(len(y_hat_batch)):\n",
        "\n",
        "            start_idx = t - self.config.n_predictions\n",
        "            start_idx = start_idx if start_idx >= 0 else 0\n",
        "\n",
        "            # predictions pertaining to a specific timestep lie along diagonal\n",
        "            y_hat_t = np.flipud(y_hat_batch[start_idx:t+1]).diagonal()\n",
        "\n",
        "            if method == 'first':\n",
        "                agg_y_hat_batch = np.append(agg_y_hat_batch, [y_hat_t[0]])\n",
        "            elif method == 'mean':\n",
        "                agg_y_hat_batch = np.append(agg_y_hat_batch, np.mean(y_hat_t))\n",
        "\n",
        "        agg_y_hat_batch = agg_y_hat_batch.reshape(len(agg_y_hat_batch), 1)\n",
        "        self.y_hat = np.append(self.y_hat, agg_y_hat_batch)\n",
        "\n",
        "    def batch_predict(self, channel):\n",
        "\n",
        "        # to trained LSTM model to predict test data arriving in batches\n",
        "\n",
        "        num_batches = int((channel.y_test.shape[0] - self.config.l_s)\n",
        "                          / self.config.batch_size)\n",
        "        if num_batches < 0:\n",
        "            raise ValueError('l_s ({}) too large for stream length {}.'\n",
        "                             .format(self.config.l_s, channel.y_test.shape[0]))\n",
        "\n",
        "        with tf.device('/GPU:0'):\n",
        "            for i in range(0, num_batches + 1):\n",
        "                prior_idx = i * self.config.batch_size\n",
        "                idx = (i + 1) * self.config.batch_size\n",
        "\n",
        "                if i + 1 == num_batches + 1:\n",
        "                    # remaining values won't necessarily equal batch size\n",
        "                    idx = channel.y_test.shape[0]\n",
        "\n",
        "                X_test_batch = channel.X_test[prior_idx:idx]\n",
        "                y_hat_batch = self.model.predict(X_test_batch)\n",
        "                self.aggregate_predictions(y_hat_batch)\n",
        "\n",
        "        self.y_hat = np.reshape(self.y_hat, (self.y_hat.size,))\n",
        "\n",
        "        channel.y_hat = self.y_hat\n",
        "\n",
        "        y_hat_df = pd.DataFrame({'Predicted': self.y_hat})\n",
        "\n",
        "        return channel\n"
      ],
      "metadata": {
        "id": "9_N4sWYi3N66"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class LSTM_NDT:\n",
        "  #for running anomaly detection over a group of channels\n",
        "    #def __init__(self, config_dict, result_path='/content/', train_path=None, test_path=None):\n",
        "    def __init__(self, config_dict, train_path=None, test_path=None):\n",
        "\n",
        "        self.train_path = train_path\n",
        "        self.test_path = test_path\n",
        "\n",
        "        self.results = []\n",
        "        self.result_df = None\n",
        "        self.chan_df = None\n",
        "\n",
        "        self.result_tracker = {\n",
        "            'true_positives': 0,\n",
        "            'false_positives': 0,\n",
        "            'false_negatives': 0\n",
        "        }\n",
        "\n",
        "        self.config = Config(config_dict)\n",
        "        self.y_hat = None\n",
        "\n",
        "        '''if not self.config.predict and self.config.use_id:\n",
        "            self.id = self.config.use_id\n",
        "        else:\n",
        "            self.id = dt.now().strftime('%Y-%m-%d_%H.%M.%S')'''\n",
        "        self.id = dt.now().strftime('%Y-%m-%d_%H.%M.%S')\n",
        "\n",
        "        # make_dirs(self.id)\n",
        "        make_dirs()\n",
        "\n",
        "        # Add logging FileHandler based on ID\n",
        "        hdlr = logging.FileHandler('logs/%s.log' % self.id)\n",
        "        formatter = logging.Formatter('%(asctime)s %(levelname)s %(message)s')\n",
        "        hdlr.setFormatter(formatter)\n",
        "        logger.addHandler(hdlr)\n",
        "\n",
        "        #self.result_path = result_path\n",
        "\n",
        "        data = pd.read_csv(train_path)\n",
        "        chan_ids = data.columns[1:].tolist()\n",
        "\n",
        "        self.chan_df = pd.DataFrame({\"chan_id\": chan_ids}, columns=[\"chan_id\"])\n",
        "\n",
        "        logger.info(\"{} channels found for processing.\".format(len(self.chan_df)))\n",
        "\n",
        "\n",
        "\n",
        "    def log_final_results(self):\n",
        "\n",
        "      # Print final results at the end of the experiment.\n",
        "\n",
        "\n",
        "      unique_indices = set()\n",
        "      total_point_anomalies = 0\n",
        "      for seq in self.result_df['anomaly_sequences']:\n",
        "        for start, end in seq:\n",
        "            for i in range(start, end + 1):\n",
        "                if i not in unique_indices:\n",
        "                    unique_indices.add(i)\n",
        "                    total_point_anomalies += 1\n",
        "\n",
        "      print('-----------------------------------')\n",
        "      print('Final Results :')\n",
        "      print('-----------------------------------')\n",
        "      print('Best model : LSTM-NDT')\n",
        "      print('Total channel sets evaluated: {}'.format(len(self.result_df)))\n",
        "      print('Avg normalized prediction error: {}'.format(self.result_df['normalized_pred_error'].mean()))\n",
        "      print('Total number of values evaluated: {}'.format(self.result_df['num_test_values'].sum()))\n",
        "      print('Total collective anomalies found: {}'.format(self.result_df['n_predicted_anoms'].sum()))\n",
        "      print('Total number of point anomalies: {}'.format(total_point_anomalies))\n",
        "      non_empty_sequences = self.result_df['anomaly_sequences'].apply(lambda seq: seq if len(seq) > 0 else None)\n",
        "      non_empty_sequences = non_empty_sequences.dropna()\n",
        "\n",
        "      if not non_empty_sequences.empty:\n",
        "        print('Anomaly sequences start/end indices:')\n",
        "        for seq in non_empty_sequences:\n",
        "            print(seq)\n",
        "      else:\n",
        "        print('No anomaly sequences found.')\n",
        "\n",
        "\n",
        "    def test(self):\n",
        "\n",
        "        #Initiate testing for all channels using saved models and GPU.\n",
        "\n",
        "        gpu_options = tf.compat.v1.GPUOptions(allow_growth=True)\n",
        "        config = tf.compat.v1.ConfigProto(gpu_options=gpu_options)\n",
        "        self.config.train=False\n",
        "        with tf.compat.v1.Session(config=config) as sess:\n",
        "            for i, row in self.chan_df.iterrows():\n",
        "                logger.info('Stream # {}: {}'.format(i + 1, row.chan_id))\n",
        "                channel = Channel(self.config, row.chan_id)\n",
        "                channel.load_data(self.train_path, self.test_path)\n",
        "\n",
        "                if self.config.predict:\n",
        "                    model = Model(self.config, self.id, channel)\n",
        "                    channel = model.batch_predict(channel)\n",
        "\n",
        "                errors = Errors(channel, self.config, self.id)\n",
        "                errors.process_batches(channel)\n",
        "\n",
        "                result_row = {\n",
        "                    'run_id': self.id,\n",
        "                    'chan_id': row.chan_id,\n",
        "                    'num_test_values': len(channel.X_test),\n",
        "                    'n_predicted_anoms': len(errors.E_seq),\n",
        "                    'normalized_pred_error': errors.normalized,\n",
        "                    'anom_scores': errors.anom_scores,\n",
        "                    'mae': model.mae,\n",
        "                    'mse': model.mse,\n",
        "                    'mape': model.mape\n",
        "                }\n",
        "\n",
        "                result_row['anomaly_sequences'] = errors.E_seq\n",
        "                self.results.append(result_row)\n",
        "\n",
        "                self.result_df = pd.DataFrame(self.results)\n",
        "\n",
        "            self.log_final_results()\n",
        "\n",
        "    def train(self):\n",
        "\n",
        "      #Train models for all channels using GPU\n",
        "      gpu_options = tf.compat.v1.GPUOptions(allow_growth=True)\n",
        "      config = tf.compat.v1.ConfigProto(gpu_options=gpu_options)\n",
        "\n",
        "      models = []\n",
        "      errors=[]\n",
        "      with tf.compat.v1.Session(config=config) as sess:\n",
        "        for i, row in self.chan_df.iterrows():\n",
        "            logger.info('Stream # {}: {}'.format(i + 1, row.chan_id))\n",
        "            channel = Channel(self.config, row.chan_id)\n",
        "            channel.load_data(self.train_path, self.test_path)\n",
        "\n",
        "            if self.config.predict:\n",
        "                model = Model(self.config, self.id, channel)\n",
        "                models.append(model)\n",
        "\n",
        "            errors_row = {\n",
        "                    'mae': model.mae,\n",
        "                    'mse': model.mse,\n",
        "                    'mape': model.mape\n",
        "                }\n",
        "            errors.append(errors_row)\n",
        "\n",
        "        errors_df = pd.DataFrame(errors)\n",
        "        mae = errors_df['mae'].mean()\n",
        "        mse = errors_df['mse'].mean()\n",
        "        mape = errors_df['mape'].mean()\n",
        "\n",
        "        means_df = pd.DataFrame({'mae': [mae], 'mse': [mse], 'mape': [mape]})\n",
        "        means_df.to_csv(os.path.join('models/', 'errors.csv'), index=False)\n",
        "\n",
        "        return [mae,mse,mape]\n",
        "\n"
      ],
      "metadata": {
        "id": "Inj0Ak2f2LrL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **GNN-GDN**"
      ],
      "metadata": {
        "id": "onsYHfXF3Fqj"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZCueG9Xv-5Ow"
      },
      "outputs": [],
      "source": [
        "class GraphLayer(MessagePassing):\n",
        "    #Graph convolutional layer implementation\n",
        "    def __init__(self, in_channels, out_channels,\n",
        "                 negative_slope=0.2, dropout=0, bias=True, inter_dim=-1):\n",
        "        super(GraphLayer, self).__init__(aggr='add')\n",
        "\n",
        "        self.in_channels = in_channels #Number of input channels/features.\n",
        "        self.out_channels = out_channels #Number of output channels/features.\n",
        "        self.negative_slope = negative_slope #LeakyReLU angle of the negative slope. Default is 0.2.\n",
        "        self.dropout = dropout\n",
        "        self.lin = Linear(in_channels, out_channels, bias=False)\n",
        "        self.a=Parameter(torch.Tensor(1, 4*out_channels))\n",
        "\n",
        "        if bias:\n",
        "            self.bias = Parameter(torch.Tensor( out_channels))\n",
        "        else:\n",
        "            self.register_parameter('bias',None)\n",
        "\n",
        "        self.reset_parameters()\n",
        "\n",
        "    def reset_parameters(self):\n",
        "        #Reset layer parameters using Xavier/Glorot and zero initialization.\n",
        "        glorot(self.lin.weight)\n",
        "\n",
        "        glorot(self.a)\n",
        "        zeros(self.bias)\n",
        "\n",
        "\n",
        "    def forward(self, batch_mat, topk_edge, embedding):\n",
        "        #Forward pass of the graph convolutional layer.\n",
        "\n",
        "        x = self.lin(batch_mat)\n",
        "        x = (x, x)\n",
        "\n",
        "        out = self.propagate(topk_edge, x=x, embedding=embedding, topk_edge=topk_edge)\n",
        "        if self.bias is not None:\n",
        "            out = out + self.bias\n",
        "\n",
        "        return out\n",
        "\n",
        "    def message(self, x_i, x_j, edge_index_i, size_i, embedding, topk_edge):\n",
        "        #Message propagation function for the graph convolutional layer.\n",
        "        embedding_j=torch.empty(size=x_i.shape)\n",
        "        embedding_i=torch.empty(size=x_i.shape)\n",
        "        for i in range(len(topk_edge[0])):\n",
        "            j_idx, i_idx=topk_edge[0][i], topk_edge[1][i]\n",
        "            embedding_j[i]=embedding[j_idx]\n",
        "            embedding_i[i]=embedding[i_idx]\n",
        "\n",
        "        git = torch.cat((x_i, embedding_i), dim=-1)\n",
        "        gjt = torch.cat((x_j, embedding_j), dim=-1)\n",
        "        gijt=torch.cat((git,gjt),dim=-1)\n",
        "        alpha= (self.a * gijt).sum(-1)\n",
        "        alpha = alpha.view(-1, 1)\n",
        "        alpha = F.leaky_relu(alpha, self.negative_slope)\n",
        "\n",
        "        alpha = softmax(alpha, edge_index_i, num_nodes=size_i)\n",
        "        alpha = F.dropout(alpha, p=self.dropout, training=self.training)\n",
        "        return x_j * alpha\n",
        "\n",
        "    def __repr__(self):\n",
        "        # String representation of the GraphLayer class\n",
        "        return '{}({}, {}, heads={})'.format(self.__class__.__name__,self.in_channels,self.out_channels, 1)\n",
        "\n",
        "class OutLayer(nn.Module):\n",
        "\n",
        "    #Output layer implementation.\n",
        "    def __init__(self, in_num, node_num, layer_num, inter_num = 512):\n",
        "        super(OutLayer, self).__init__()\n",
        "        modules = []\n",
        "\n",
        "        for i in range(layer_num):\n",
        "            if i == layer_num-1:\n",
        "                modules.append(nn.Linear( in_num if layer_num == 1 else inter_num, 1))\n",
        "            else:\n",
        "                layer_in_num = in_num if i == 0 else inter_num\n",
        "                modules.append(nn.Linear( layer_in_num, inter_num ))\n",
        "                modules.append(nn.BatchNorm1d(inter_num))\n",
        "                modules.append(nn.ReLU())\n",
        "        self.mlp = nn.ModuleList(modules)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Forward pass of the output layer.\n",
        "        out = x\n",
        "        for mod in self.mlp:\n",
        "            if isinstance(mod, nn.BatchNorm1d):\n",
        "                out = out.permute(0,2,1)\n",
        "                out = mod(out)\n",
        "                out = out.permute(0,2,1)\n",
        "            else:\n",
        "                out = mod(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "\n",
        "class GNNLayer(nn.Module):\n",
        "    #GNN layer implementation.\n",
        "    def __init__(self, in_channel, out_channel, inter_dim):\n",
        "        super(GNNLayer, self).__init__()\n",
        "        self.graph = GraphLayer(in_channel, out_channel, inter_dim=inter_dim)\n",
        "        self.bn = nn.BatchNorm1d(out_channel)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.leaky_relu = nn.LeakyReLU()\n",
        "\n",
        "    def forward(self, batch_mat,topk_edge, embedding):\n",
        "        #Forward pass of the GNN layer\n",
        "        out= self.graph(batch_mat, topk_edge, embedding)\n",
        "        out = self.bn(out)\n",
        "        return self.relu(out)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OxaKbOGJ_Bd4"
      },
      "outputs": [],
      "source": [
        "class GDN(nn.Module):\n",
        "    def __init__(self, full_edges, node_num, embed_dim=64, out_layer_inter_dim=256, input_dim=10, out_layer_num=1, topk=20):\n",
        "        #Graph Deviation Network layer implementation.\n",
        "        super(GDN,self).__init__()\n",
        "        self.embedding = nn.Embedding(node_num,embed_dim)\n",
        "        self.bn_outlayer_in = nn.BatchNorm1d(embed_dim)\n",
        "        self.gnn_layer=GNNLayer(input_dim,embed_dim,2*embed_dim)\n",
        "        self.topk = topk\n",
        "        self.edges=full_edges\n",
        "        self.out_layer = OutLayer(embed_dim, node_num, out_layer_num, inter_num = out_layer_inter_dim)\n",
        "        self.dp = nn.Dropout(0.2)\n",
        "\n",
        "    def init_params(self):\n",
        "        nn.init.kaiming_uniform_(self.embedding.weight, a=math.sqrt(5))\n",
        "\n",
        "    def forward(self, batch_tensor, org_edge_index):\n",
        "        #Forward pass of the GDN model.\n",
        "\n",
        "        batch_mat = batch_tensor.clone().detach()\n",
        "        batch_size, node_num, feature_len = batch_tensor.shape\n",
        "        batch_mat = batch_mat.view(-1, feature_len).contiguous()\n",
        "        scalar_data_num=batch_mat.shape[0]\n",
        "        edge_num = self.edges.shape[1]\n",
        "        all_embeddings = self.embedding(torch.arange(node_num))\n",
        "        weights = all_embeddings.detach().clone()\n",
        "        cos_ji_mat = torch.matmul(weights, weights.T)\n",
        "        normed_mat = torch.matmul(weights.norm(dim=-1).view(-1,1), weights.norm(dim=-1).view(1,-1))\n",
        "        cos_ji_mat = cos_ji_mat / normed_mat\n",
        "        topk_indices_ji = torch.topk(cos_ji_mat, self.topk, dim=-1)[1]\n",
        "\n",
        "        gated_i = torch.arange(0, node_num).T.unsqueeze(1).repeat(1, self.topk).flatten().unsqueeze(0)\n",
        "        gated_j = topk_indices_ji.flatten().unsqueeze(0)\n",
        "        topk_edge = torch.cat((gated_j, gated_i), dim=0)\n",
        "\n",
        "        gcn_out = self.gnn_layer(batch_mat, topk_edge, embedding=all_embeddings)\n",
        "        x = gcn_out.view(batch_size, node_num, -1)\n",
        "\n",
        "        indexes = torch.arange(0,node_num)\n",
        "        out = torch.mul(x, self.embedding(indexes))\n",
        "\n",
        "        out = out.permute(0,2,1)\n",
        "        out = F.relu(self.bn_outlayer_in(out))\n",
        "        out = out.permute(0,2,1)\n",
        "        out = self.dp(out)\n",
        "        out = self.out_layer(out)\n",
        "        out = out.view(-1, node_num)\n",
        "\n",
        "        return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SMSS37GA_JNP"
      },
      "outputs": [],
      "source": [
        "def err_scores(test_result, val_result):\n",
        "    #Compute error scores for test and validation results.\n",
        "    np_test_result = np.array(test_result)\n",
        "    np_val_result = np.array(val_result)\n",
        "\n",
        "    all_scores =  None\n",
        "    all_normals = None\n",
        "    feature_num = np_test_result.shape[-1]\n",
        "\n",
        "    labels = np_test_result[2, :, 0].tolist()\n",
        "\n",
        "    for i in range(feature_num):\n",
        "        test_re_list = np_test_result[:2,:,i]\n",
        "        val_re_list = np_val_result[:2,:,i]\n",
        "\n",
        "        scores = s_err_scores(test_re_list, val_re_list)\n",
        "        normal_dist = s_err_scores(val_re_list, val_re_list)\n",
        "\n",
        "        if all_scores is None:\n",
        "            all_scores = scores\n",
        "            all_normals = normal_dist\n",
        "        else:\n",
        "            all_scores = np.vstack((\n",
        "                all_scores,\n",
        "                scores\n",
        "            ))\n",
        "            all_normals = np.vstack((\n",
        "                all_normals,\n",
        "                normal_dist\n",
        "            ))\n",
        "\n",
        "    return all_scores, all_normals\n",
        "\n",
        "def s_err_scores(test_res, val_res):\n",
        "    #Compute Smoothed  error scores based on test and validation results\n",
        "    test_predict, test_gt = test_res\n",
        "    val_predict, val_gt = val_res\n",
        "\n",
        "    n_err_mid, n_err_iqr = err_median_and_iqr(test_predict, test_gt)\n",
        "\n",
        "    test_delta = np.abs(np.subtract(\n",
        "                        np.array(test_predict).astype(np.float64),\n",
        "                        np.array(test_gt).astype(np.float64)\n",
        "                    ))\n",
        "    epsilon=1e-2\n",
        "\n",
        "    err_scores = (test_delta - n_err_mid) / ( np.abs(n_err_iqr) +epsilon)\n",
        "\n",
        "    smoothed_err_scores = np.zeros(err_scores.shape)\n",
        "    before_num = 3\n",
        "    for i in range(before_num, len(err_scores)):\n",
        "        smoothed_err_scores[i] = np.mean(err_scores[i-before_num:i+1])\n",
        "\n",
        "\n",
        "    return smoothed_err_scores\n",
        "\n",
        "\n",
        "def detect_anomalies(total_err_scores, test_result, topk=1):\n",
        "\n",
        "    #Compute the best performance metrics based on error scores and test results and threshold\n",
        "    total_features = total_err_scores.shape[0]\n",
        "\n",
        "    topk_indices = np.argpartition(total_err_scores, range(total_features-topk-1, total_features), axis=0)[-topk:]\n",
        "\n",
        "    total_topk_err_scores = []\n",
        "    total_topk_err_scores = np.sum(np.take_along_axis(total_err_scores, topk_indices, axis=0), axis=0)\n",
        "\n",
        "    thresholds = get_threshold(total_topk_err_scores, 400)\n",
        "    pred_values = np.array(test_result[0])\n",
        "    true_values = np.array(test_result[1])\n",
        "\n",
        "    pred_values = pred_values.reshape(-1, pred_values.shape[-1])\n",
        "    true_values = true_values.reshape(-1, true_values.shape[-1])\n",
        "\n",
        "    pred_values = pred_values[:, :total_topk_err_scores.shape[0]]\n",
        "    true_values = true_values[:, :total_topk_err_scores.shape[0]]\n",
        "\n",
        "    mae = mean_absolute_error(true_values, pred_values)\n",
        "    mse = mean_squared_error(true_values, pred_values)\n",
        "    mape = np.mean(np.abs((true_values - pred_values) / true_values)) * 100.0\n",
        "\n",
        "    th_i = np.argmax(total_topk_err_scores)\n",
        "    threshold = thresholds[th_i]\n",
        "\n",
        "    pred_labels = np.zeros(len(total_topk_err_scores))\n",
        "    pred_labels[total_topk_err_scores > threshold] = 1\n",
        "    num_anomalies = np.sum(pred_labels == 1)\n",
        "    anomaly_indices = np.where(pred_labels == 1)[0]\n",
        "\n",
        "    # Grouping the consecutive indices\n",
        "    groups = []\n",
        "    current_start = anomaly_indices[0]\n",
        "    for i in range(1, len(anomaly_indices)):\n",
        "      if anomaly_indices[i] != anomaly_indices[i-1] + 1:\n",
        "        current_end = anomaly_indices[i-1]\n",
        "        groups.append((current_start, current_end))\n",
        "        current_start = anomaly_indices[i]\n",
        "\n",
        "    current_end = anomaly_indices[-1]\n",
        "    groups.append((current_start, current_end))\n",
        "    print('-----------------------------------')\n",
        "    print('Final Results :')\n",
        "    print('-----------------------------------')\n",
        "    print('Best model : GDN')\n",
        "    print(\"Number of point anomalies:\", num_anomalies)\n",
        "    print(\"number of collective anomalies\", len(groups))\n",
        "    print(\"Grouped anomaly indices:\")\n",
        "    for start, end in groups:\n",
        "      print([start, end])\n",
        "    return mae, mse, mape, threshold\n",
        "\n",
        "def get_threshold(scores, th_steps):\n",
        "    #Compute the threshold values based on scores\n",
        "    padding_list = [0]*(len(scores) - len(scores))\n",
        "    if len(padding_list) > 0:\n",
        "        scores = padding_list + scores\n",
        "\n",
        "    scores_sorted = rankdata(scores, method='ordinal')\n",
        "    th_steps = th_steps\n",
        "    th_vals = np.array(range(th_steps)) * 1.0 / th_steps\n",
        "\n",
        "    thresholds = [None] * th_steps\n",
        "    for i in range(th_steps):\n",
        "        score_index = scores_sorted.tolist().index(int(th_vals[i] * len(scores)+1))\n",
        "        thresholds[i] = scores[score_index]\n",
        "\n",
        "    return thresholds\n",
        "\n",
        "def err_median_and_iqr(predicted, groundtruth):\n",
        "    #Compute the median and IQR (Interquartile Range) of the errors.\n",
        "\n",
        "    np_arr = np.abs(np.subtract(np.array(predicted), np.array(groundtruth)))\n",
        "    err_median = np.median(np_arr)\n",
        "    err_iqr = iqr(np_arr)\n",
        "\n",
        "    return err_median, err_iqr"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Kg-b4L0Y_R1T"
      },
      "outputs": [],
      "source": [
        "#Loss functions\n",
        "def loss_mse(y_pred, y_true):\n",
        "    loss = F.mse_loss(y_pred, y_true, reduction='mean')\n",
        "\n",
        "    return loss\n",
        "\n",
        "def loss_mae(y_pred, y_true):\n",
        "    loss = F.l1_loss(y_pred, y_true, reduction='mean')\n",
        "    return loss\n",
        "\n",
        "def loss_mape(y_pred, y_true):\n",
        "    epsilon = 1e-7\n",
        "    loss = torch.abs((y_true - y_pred) / (y_true + epsilon))\n",
        "    loss = torch.mean(loss) * 100.0\n",
        "    return loss\n",
        "\n",
        "\n",
        "\n",
        "def train(model, config, train_dataloader, val_dataloader, nodes_list, test_dataloader, test_dataset, train_dataset, full_edges):\n",
        "\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=config['decay'])\n",
        "    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1) # Example\n",
        "\n",
        "    now = time.time()\n",
        "\n",
        "    train_loss_list = []\n",
        "    val_loss_list = []\n",
        "\n",
        "    acu_loss = 0\n",
        "    acu_mse = 0\n",
        "    acu_mape = 0\n",
        "    val_acu_loss = 0\n",
        "    min_loss = 1e+8\n",
        "    mse = 0\n",
        "    mae = 0\n",
        "    mape = 0\n",
        "\n",
        "    i = 0\n",
        "    epoch = config['epoch']\n",
        "    early_stop_win = 15\n",
        "\n",
        "    model.train()\n",
        "\n",
        "    log_interval = 1000\n",
        "    stop_improve_count = 0\n",
        "\n",
        "    dataloader = train_dataloader\n",
        "\n",
        "    logger = logging.getLogger(__name__)\n",
        "    logger.setLevel(logging.INFO)\n",
        "    formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
        "\n",
        "    console_handler = logging.StreamHandler()\n",
        "    console_handler.setFormatter(formatter)\n",
        "    logger.addHandler(console_handler)\n",
        "\n",
        "    for i_epoch in range(epoch):\n",
        "\n",
        "        acu_loss = 0\n",
        "        val_acu_loss = 0\n",
        "        model.train()\n",
        "        scheduler.step()\n",
        "\n",
        "        for x, labels, _ in dataloader:\n",
        "            _start = time.time()\n",
        "\n",
        "            x, labels = [item.float() for item in [x, labels]]\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            out = model(x, full_edges).float()\n",
        "            loss = loss_mae(out, labels)\n",
        "            mse=loss_mse(out,labels)\n",
        "            mape=loss_mape(out,labels)\n",
        "\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "\n",
        "            acu_loss += loss.item()\n",
        "            acu_mse += mse.item()\n",
        "            acu_mape += mape.item()\n",
        "\n",
        "            i += 1\n",
        "\n",
        "        # append average training loss for the epoch to train_loss_list\n",
        "        train_loss_list.append(acu_loss/len(dataloader))\n",
        "\n",
        "        logger.info('epoch ({}/{}), Loss: {:.8f}, ACU_loss: {:.8f}'.format(\n",
        "            i_epoch, epoch, acu_loss/len(dataloader), acu_loss))\n",
        "\n",
        "        # use val dataset to judge\n",
        "        if val_dataloader is not None:\n",
        "\n",
        "            val_loss, val_result = test(model, val_dataloader,full_edges)\n",
        "\n",
        "            val_acu_loss += val_loss\n",
        "\n",
        "            if val_loss < min_loss:\n",
        "                torch.save(model.state_dict(),'model.pt')\n",
        "\n",
        "                min_loss = val_loss\n",
        "                stop_improve_count = 0\n",
        "                mae= acu_loss/len(dataloader)\n",
        "                mse= acu_mse/len(dataloader)\n",
        "                mape=acu_mape/len(dataloader)\n",
        "            else:\n",
        "                stop_improve_count += 1\n",
        "\n",
        "\n",
        "            if stop_improve_count >= early_stop_win:\n",
        "                break\n",
        "\n",
        "        # append average validation loss for the epoch to val_loss_list\n",
        "        if val_dataloader is not None:\n",
        "            val_loss_list.append(val_acu_loss/len(val_dataloader))\n",
        "\n",
        "    return train_loss_list, val_loss_list, mae,mse.item(),mape.item()\n",
        "\n",
        "\n",
        "def test(model, dataloader, full_edges):\n",
        "    # test\n",
        "    loss_func = nn.MSELoss(reduction='mean')\n",
        "\n",
        "    test_loss_list = []\n",
        "    now = time.time()\n",
        "\n",
        "    test_predicted_list = []\n",
        "    test_ground_list = []\n",
        "    test_labels_list = []\n",
        "\n",
        "    t_test_predicted_list = []\n",
        "    t_test_ground_list = []\n",
        "    t_test_labels_list = []\n",
        "\n",
        "    test_len = len(dataloader)\n",
        "\n",
        "    model.eval()\n",
        "\n",
        "    i = 0\n",
        "    acu_loss = 0\n",
        "    for x, y, labels in dataloader:\n",
        "        x, y, labels= [item.float() for item in [x, y, labels]]\n",
        "\n",
        "        with torch.no_grad():\n",
        "            predicted = model(x, full_edges).float()\n",
        "            loss = loss_mae(predicted, y)\n",
        "            labels = labels.unsqueeze(1).repeat(1, predicted.shape[1])\n",
        "\n",
        "            if len(t_test_predicted_list) <= 0:\n",
        "                t_test_predicted_list = predicted\n",
        "                t_test_ground_list = y\n",
        "                t_test_labels_list = labels\n",
        "            else:\n",
        "                t_test_predicted_list = torch.cat((t_test_predicted_list, predicted), dim=0)\n",
        "                t_test_ground_list = torch.cat((t_test_ground_list, y), dim=0)\n",
        "                t_test_labels_list = torch.cat((t_test_labels_list, labels), dim=0)\n",
        "\n",
        "        test_loss_list.append(loss.item())\n",
        "        acu_loss += loss.item()\n",
        "\n",
        "        i += 1\n",
        "\n",
        "        if i % 100 == 0:\n",
        "            logging.info('Processed {} batches.'.format(i))\n",
        "\n",
        "    test_predicted_list = t_test_predicted_list.tolist()\n",
        "    test_ground_list = t_test_ground_list.tolist()\n",
        "    test_labels_list = t_test_labels_list.tolist()\n",
        "    avg_loss = sum(test_loss_list)/len(test_loss_list)\n",
        "    logging.info('Test completed. Average loss: {}'.format(avg_loss))\n",
        "\n",
        "    return avg_loss, [test_predicted_list, test_ground_list, test_labels_list]\n",
        "\n",
        "\n",
        "def log_final_results(test_result, val_result, report):\n",
        "    feature_num = len(test_result[0][0])\n",
        "    np_test_result = np.array(test_result)\n",
        "    np_val_result = np.array(val_result)\n",
        "    test_scores, normal_scores = err_scores(test_result, val_result)\n",
        "    info = detect_anomalies(test_scores, np_test_result, topk=1)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rrWjuwdo_f8X"
      },
      "outputs": [],
      "source": [
        "\n",
        "class TimeDataset(Dataset):\n",
        "\n",
        "    def __init__(self, data_df, mode, config):\n",
        "        self.data_df=data_df\n",
        "        self.data_df = self.data_df.astype('float64')\n",
        "        self.config=config\n",
        "        self.mode=mode\n",
        "\n",
        "        self.feature, self.label, self.attack = self.process()\n",
        "\n",
        "    def process(self):\n",
        "        win=self.config['slide_win']\n",
        "        stride=self.config['slide_stride']\n",
        "\n",
        "        if self.mode=='test':\n",
        "\n",
        "            attack_col=torch.tensor(np.array(self.data_df['attack']), dtype=torch.float64)\n",
        "            self.data_df=self.data_df.drop(columns=['attack'])\n",
        "\n",
        "        num_nodes=len(self.data_df.columns)\n",
        "        timestamp_len=len(self.data_df.iloc[:,2])\n",
        "\n",
        "        ran=range(win,timestamp_len,stride) if self.mode =='train' else range(win,timestamp_len)\n",
        "        data_num=len(ran)\n",
        "        feature=torch.zeros((data_num,num_nodes,win))\n",
        "        label=torch.zeros((data_num,num_nodes))\n",
        "        attack=torch.zeros((data_num))\n",
        "\n",
        "        for cnt,i in enumerate(ran):\n",
        "            mat_i=torch.zeros((num_nodes,win))\n",
        "            label_i=torch.zeros((num_nodes))\n",
        "            for j in range(num_nodes):\n",
        "                column = torch.tensor(np.array(self.data_df.iloc[:,j]), dtype=torch.float64)\n",
        "\n",
        "                mat_i[j]=column[i-win:i]\n",
        "                label_i[j]=column[i]\n",
        "\n",
        "                if j==0 and self.mode=='test':\n",
        "                    attack[cnt]=attack_col[i]\n",
        "\n",
        "            feature[cnt]=mat_i\n",
        "            label[cnt]=label_i\n",
        "\n",
        "        return feature, label, attack\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.feature)\n",
        "\n",
        "    def __getitem__(self,idx):\n",
        "\n",
        "        return self.feature[idx], self.label[idx], self.attack[idx]\n",
        "\n",
        "def train_val_loader(train_dataset, batch, val_ratio=0.1, num_workers=0, seed=42):\n",
        "    dataset_len = len(train_dataset)\n",
        "    train_use_len = int(dataset_len * (1 - val_ratio))\n",
        "    val_use_len = int(dataset_len * val_ratio)\n",
        "\n",
        "    random.seed(seed)\n",
        "    val_start_index = random.randrange(train_use_len)\n",
        "    indices = torch.arange(dataset_len)\n",
        "\n",
        "    train_sub_indices = torch.cat([indices[:val_start_index], indices[val_start_index+val_use_len:]])\n",
        "    train_subset = Subset(train_dataset, train_sub_indices)\n",
        "\n",
        "    val_sub_indices = indices[val_start_index:val_start_index+val_use_len]\n",
        "    val_subset = Subset(train_dataset, val_sub_indices)\n",
        "\n",
        "    train_dataloader = DataLoader(train_subset, batch_size=batch, shuffle=True, num_workers=num_workers)\n",
        "    val_dataloader = DataLoader(val_subset, batch_size=batch, shuffle=False, num_workers=num_workers)\n",
        "\n",
        "    return train_dataloader, val_dataloader\n",
        "\n",
        "def get_full_edges(csv_path):\n",
        "    with open(csv_path, 'r') as f:\n",
        "        reader = csv.reader(f)\n",
        "        nodes_list = next(reader)[1:]\n",
        "\n",
        "    full_edges = []\n",
        "    for i in range(len(nodes_list)):\n",
        "        for j in range(len(nodes_list)):\n",
        "            if i == j:\n",
        "                continue\n",
        "            full_edges.append([i, j])\n",
        "\n",
        "    full_edges = torch.tensor(full_edges).T\n",
        "    return full_edges,nodes_list\n",
        "\n",
        "def load_data(config,train_path, test_path=None):\n",
        "    if test_path is None:\n",
        "        # Only train_path is given, read the CSV file and split into train and test\n",
        "        df = pd.read_csv(train_path, index_col=0)\n",
        "        df = df.rename(columns={df.columns[0]: \"timestamp\"})\n",
        "        df[\"timestamp\"] = range(len(df))\n",
        "        train_df, test_df = train_test_split(df, test_size=0.2, shuffle=False)\n",
        "    else:\n",
        "        # Both train_path and test_path are given, read the respective CSV files\n",
        "        train_df = pd.read_csv(train_path, index_col=0)\n",
        "        train_df = train_df.rename(columns={train_df.columns[0]: \"timestamp\"})\n",
        "        train_df[\"timestamp\"] = range(len(train_df))\n",
        "\n",
        "        test_df = pd.read_csv(test_path, index_col=0)\n",
        "        test_df = test_df.rename(columns={test_df.columns[0]: \"timestamp\"})\n",
        "        test_df[\"timestamp\"] = range(len(test_df))\n",
        "\n",
        "    # normalization\n",
        "    scaler = RobustScaler()\n",
        "    numerical_columns = train_df.select_dtypes(include=[np.number]).columns.tolist()\n",
        "    train_df[numerical_columns] = scaler.fit_transform(train_df[numerical_columns])\n",
        "    test_df[numerical_columns] = scaler.transform(test_df[numerical_columns])\n",
        "\n",
        "    train_dataset = TimeDataset(train_df, 'train', config)\n",
        "    test_dataset = TimeDataset(test_df, 'train', config)\n",
        "\n",
        "    train_dataloader, val_dataloader = train_val_loader(train_dataset, config['batch'], config['val_ratio'])\n",
        "    test_dataloader = DataLoader(test_dataset, batch_size=config['batch'], shuffle=False, num_workers=8)\n",
        "\n",
        "    return train_dataset,test_dataset,train_dataloader, val_dataloader, test_dataloader"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Prophet**"
      ],
      "metadata": {
        "id": "6SwzaUqQ3pze"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class PROPHET:\n",
        "    def __init__(self, dataset):\n",
        "        self.dataset = dataset\n",
        "        self.df_entire = None\n",
        "        self.prophet_model = None\n",
        "        self.entire_dataframe = None\n",
        "        self.columns=None\n",
        "        self.target_column=None\n",
        "\n",
        "    def col_to_ds(self,target_column):\n",
        "      #takes csv file and renames first date column into 'ds' and target column as 'y'\n",
        "        file_path = f'/content/{self.dataset}.csv'\n",
        "        self.df_entire = pd.read_csv(file_path)\n",
        "        first_column_name = self.df_entire.columns[0]\n",
        "\n",
        "        self.df_entire = self.df_entire.rename(columns={first_column_name: 'ds', target_column: 'y'})\n",
        "        self.df_entire['ds'] = pd.to_datetime(self.df_entire['ds'])\n",
        "\n",
        "    def train(self,columns):\n",
        "      #Adds regression based on input columns\n",
        "        self.prophet_model = Prophet()\n",
        "        for i in range(len(columns)):\n",
        "            print(\"**********************Column name for regression: \",columns[i])\n",
        "            self.prophet_model.add_regressor(columns[i], standardize=False)\n",
        "        self.prophet_model.fit(self.df_entire)\n",
        "\n",
        "    def channel_predict(self, column):\n",
        "      #creates univariate models for each regression column for future forecasting\n",
        "        df_column = self.df_entire[['ds', column]].rename(columns={column: 'y'})\n",
        "\n",
        "        m_column = Prophet()\n",
        "        m_column.fit(df_column)\n",
        "\n",
        "        future_column = m_column.make_future_dataframe(periods=365)\n",
        "        forecasted_column = m_column.predict(future_column)\n",
        "\n",
        "        fc = forecasted_column.drop(axis=1, labels=[\n",
        "            'trend', 'yhat_lower', 'yhat_upper', 'trend_lower', 'trend_upper', 'additive_terms',\n",
        "            'additive_terms_lower', 'additive_terms_upper', 'weekly', 'weekly_lower', 'weekly_upper', 'multiplicative_terms', 'multiplicative_terms_lower',\n",
        "            'multiplicative_terms_upper'\n",
        "        ])\n",
        "\n",
        "        fc.rename(columns={'yhat': column}, inplace=True)\n",
        "\n",
        "        return fc\n",
        "\n",
        "    def merge_forecasts(self, forecasts):\n",
        "      #combines forecsted values with previous dataset based on date\n",
        "      df_forecast = forecasts[0]\n",
        "      for i in range(1, len(forecasts)):\n",
        "         df_forecast = pd.merge(df_forecast, forecasts[i], on='ds')\n",
        "         # Drop duplicate columns\n",
        "         df_forecast = df_forecast.loc[:, ~df_forecast.columns.duplicated()]\n",
        "      return df_forecast\n",
        "\n",
        "    def aggregate_predictions(self, df_forecast):\n",
        "      #predicted for future 365 points for univariate and combined with dataframe\n",
        "        df_forecast_last_one_year = df_forecast[-365:]\n",
        "        print(self.df_entire.index.is_unique)\n",
        "        print(df_forecast_last_one_year.index.is_unique)\n",
        "        self.df_entire.reset_index(drop=True, inplace=True)\n",
        "        df_forecast_last_one_year.reset_index(drop=True, inplace=True)\n",
        "        self.entire_dataframe = pd.concat([self.df_entire, df_forecast_last_one_year], ignore_index=True)\n",
        "\n",
        "\n",
        "    def predict_target(self):\n",
        "      #predicts multivariate target column values\n",
        "        forecasting_for_oneyear = self.prophet_model.predict(self.entire_dataframe)\n",
        "        return forecasting_for_oneyear\n",
        "\n",
        "    def evaluate_performance(self, forecasted_data):\n",
        "      #evaluates MSE,MAE,MAPE scores\n",
        "        performance = pd.merge(self.df_entire, forecasted_data[['ds', 'yhat', 'yhat_lower', 'yhat_upper']], on='ds')\n",
        "\n",
        "\n",
        "        performance_MAE = mean_absolute_error(performance['y'], performance['yhat'])\n",
        "\n",
        "        performance_MAPE = mean_absolute_percentage_error(performance['y'], performance['yhat'])\n",
        "\n",
        "        performance_MSE = mean_squared_error(performance['y'], performance['yhat'])\n",
        "\n",
        "        return performance_MAE,performance_MSE, performance_MAPE\n",
        "\n",
        "    def log_final_results(self,forecasted_data):\n",
        "      #Prints all anomalies and their indices\n",
        "        performance = pd.merge(self.df_entire, forecasted_data[['ds', 'yhat', 'yhat_lower', 'yhat_upper']], on='ds')\n",
        "\n",
        "        performance['anomaly'] = performance.apply(lambda rows: 1 if ((rows.y < rows.yhat_lower) or (rows.y > rows.yhat_upper)) else 0, axis=1)\n",
        "        print('-----------------------------------')\n",
        "        print('Final Results :')\n",
        "        print('-----------------------------------')\n",
        "        print('Best model : PROPHET')\n",
        "        anomalies = performance[performance['anomaly']==1].sort_values(by='ds')\n",
        "        print(\"Total number of point anomalies: \",anomalies['ds'].count())\n",
        "        pd.set_option('display.max_columns', None)\n",
        "\n",
        "        anomaly_indices = anomalies.index.tolist()\n",
        "        continuous_anomalies = []\n",
        "        start_index = None\n",
        "\n",
        "        for i in range(len(anomaly_indices)):\n",
        "          if i == 0 or anomaly_indices[i] != anomaly_indices[i-1] + 1:\n",
        "            if start_index is not None:\n",
        "              if start_index == prev_index:\n",
        "                continuous_anomalies.append(start_index)\n",
        "              else:\n",
        "                continuous_anomalies.append([start_index, prev_index])\n",
        "            start_index = anomaly_indices[i]\n",
        "          prev_index = anomaly_indices[i]\n",
        "\n",
        "        if start_index is not None:\n",
        "          if start_index == prev_index:\n",
        "            continuous_anomalies.append(start_index)\n",
        "          else:\n",
        "            continuous_anomalies.append([start_index, prev_index])\n",
        "        print(\"Total number of collective anomalies : \", len(continuous_anomalies))\n",
        "        print(\"Indices of anomalies: \")\n",
        "        for i in range(0, len(continuous_anomalies), 10):\n",
        "          line = continuous_anomalies[i:i+10]\n",
        "          print(line)"
      ],
      "metadata": {
        "id": "MB7GMQq2HPJJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**MAIN()**"
      ],
      "metadata": {
        "id": "lVdB9gvP4oaL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import threading\n",
        "import warnings\n",
        "tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)\n",
        "if __name__ == '__main__':\n",
        "\n",
        "\n",
        "    # Disable all warnings\n",
        "    warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "    # Set TensorFlow to use GPU\n",
        "    os.environ['CUDA_VISIBLE_DEVICES'] = '0'  # Replace '0' with the GPU index you want to use\n",
        "\n",
        "    # Enable GPU memory growth to allocate memory on-demand\n",
        "    gpus = tf.config.list_physical_devices('GPU')\n",
        "    if gpus:\n",
        "        try:\n",
        "            for gpu in gpus:\n",
        "                tf.config.experimental.set_memory_growth(gpu, True)\n",
        "            logical_gpus = tf.config.list_logical_devices('GPU')\n",
        "            print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
        "        except RuntimeError as e:\n",
        "            print(e)\n",
        "    #os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
        "    dataset = 'stock_data'\n",
        "    train_path = \"stock_data.csv\"\n",
        "    test_path = None\n",
        "\n",
        "    target_column=input(\"Enter the target column: \")\n",
        "    columns = input(\"Enter the columns (comma-separated) you want to add regression for: \").split(\",\")\n",
        "\n",
        "    config_lstmndt = {\n",
        "        \"train\": False,\n",
        "        \"predict\": True,\n",
        "        \"batch_size\": 70,\n",
        "        \"window_size\": 30,\n",
        "        \"smoothing_perc\": 0.05,\n",
        "        \"error_buffer\": 100,\n",
        "        \"loss_metric\": \"mae\",\n",
        "        \"optimizer\": \"adam\",\n",
        "        \"validation_split\": 0.2,\n",
        "        \"dropout\": 0.3,\n",
        "        \"lstm_batch_size\": 64,\n",
        "        \"epochs\": 35,\n",
        "        \"layers\": [80, 80],\n",
        "        \"patience\": 10,\n",
        "        \"min_delta\": 0.0003,\n",
        "        \"l_s\": 250,\n",
        "        \"n_predictions\": 10,\n",
        "        \"p\": 0.13\n",
        "    }\n",
        "\n",
        "    config_gdn = {\n",
        "        'slide_win': 15,\n",
        "        'slide_stride': 5,\n",
        "        'batch': 128,\n",
        "        'dim': 64,\n",
        "        'val_ratio': 0.1,\n",
        "        'topk': 2,  # including itself.\n",
        "        'out_layer_num': 1,\n",
        "        'out_layer_inter_dim': 256,\n",
        "        'decay': 0.001,\n",
        "        'epoch': 100,\n",
        "        'report': 'best'  # or 'val'\n",
        "    }\n",
        "\n",
        "    full_edges, nodes_list = get_full_edges(train_path)\n",
        "    train_dataset,test_dataset,train_dataloader, val_dataloader, test_dataloader = load_data(config=config_gdn,train_path=train_path, test_path=test_path)\n",
        "\n",
        "\n",
        "    prophet_model = PROPHET(dataset)\n",
        "    prophet_model.col_to_ds(target_column)\n",
        "\n",
        "\n",
        "    # Get the columns from the user\n",
        "\n",
        "\n",
        "    lstmndt_model = LSTM_NDT(config_dict=config_lstmndt, train_path=train_path, test_path=test_path)\n",
        "    gdn_model = GDN(full_edges, len(nodes_list),\n",
        "                    embed_dim=config_gdn['dim'],\n",
        "                    input_dim=config_gdn['slide_win'],\n",
        "                    out_layer_num=config_gdn['out_layer_num'],\n",
        "                    out_layer_inter_dim=config_gdn['out_layer_inter_dim'],\n",
        "                    topk=config_gdn['topk'])\n",
        "\n",
        "    def run_lstmndt():\n",
        "        global mae_lstmndt, mse_lstmndt, mape_lstmndt\n",
        "        if config_lstmndt['train']== True :\n",
        "          mae_lstmndt, mse_lstmndt, mape_lstmndt = lstmndt_model.train()\n",
        "\n",
        "        else:\n",
        "          errors_df = pd.read_csv('models/errors.csv')\n",
        "          mae_lstmndt = errors_df['mae'].values[0]\n",
        "          mse_lstmndt = errors_df['mse'].values[0]\n",
        "          mape_lstmndt = errors_df['mape'].values[0]\n",
        "        print(\"Training LSTM-NDT : complete\")\n",
        "\n",
        "    def run_gdn():\n",
        "        global train_log, val_log, mae_gdn, mse_gdn, mape_gdn\n",
        "\n",
        "        logging.basicConfig(filename='training.log', level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
        "        logging.info('Loading data and configuring model')\n",
        "        train_log, val_log, mae_gdn, mse_gdn, mape_gdn = train(gdn_model, config_gdn, train_dataloader, val_dataloader,\n",
        "                                                              nodes_list, test_dataloader, test_dataset,\n",
        "                                                              train_dataset, full_edges)\n",
        "        print(\"Training GDN : complete\")\n",
        "\n",
        "\n",
        "    def run_prophet():\n",
        "        global forecasting_for_oneyear,mae_p,mse_p,mape_p\n",
        "        prophet_model.train(columns)\n",
        "        forecasts = []\n",
        "        for column in columns:\n",
        "          fc = prophet_model.channel_predict(column)\n",
        "          forecasts.append(fc)\n",
        "        df_forecast = prophet_model.merge_forecasts(forecasts)\n",
        "        prophet_model.aggregate_predictions(df_forecast)\n",
        "        forecasting_for_oneyear = prophet_model.predict_target()\n",
        "        mae_p,mse_p,mape_p=prophet_model.evaluate_performance(forecasting_for_oneyear)\n",
        "        print(\"Training LSTM-NDT : complete\")\n",
        "\n",
        "\n",
        "\n",
        "    start_time = time.time()\n",
        "    t1 = threading.Thread(target=run_lstmndt)\n",
        "    t2 = threading.Thread(target=run_gdn)\n",
        "    t3 = threading.Thread(target=run_prophet)\n",
        "\n",
        "    # Start the threads\n",
        "    t1.start()\n",
        "    t2.start()\n",
        "    t3.start()\n",
        "    # Wait for the threads to finish\n",
        "    t1.join()\n",
        "    t2.join()\n",
        "    t3.join()\n",
        "\n",
        "    '''if mae_gdn < mae_p and mae_gdn < mae_lstmndt :\n",
        "        # Testing for GDN model\n",
        "        _, test_result = test(gdn_model, test_dataloader, full_edges)\n",
        "        _, val_result = test(gdn_model, val_dataloader, full_edges)\n",
        "        log_final_results(test_result, val_result, config_gdn['report'])\n",
        "\n",
        "        print (\"mae : \", mae_gdn)\n",
        "        print (\"mse : \", mse_gdn)\n",
        "        print (\"mape : \", mape_gdn)\n",
        "\n",
        "    elif mae_lstmndt < mae_gdn and mae_lstmndt < mae_p  :\n",
        "        # Testing for LSTMNDT model\n",
        "        lstmndt_model.test()\n",
        "        print (\"mae : \", mae_lstmndt)\n",
        "        print (\"mse : \", mse_lstmndt)\n",
        "        print (\"mape : \", mape_lstmndt)\n",
        "\n",
        "    elif mae_p < mae_gdn and mae_p < mae_lstmndt :\n",
        "        # testing for Prophet model\n",
        "        prophet_model.log_final_results(forecasting_for_oneyear)\n",
        "        print (\"mae : \", mae_p)\n",
        "        print (\"mse : \", mse_p)\n",
        "        print (\"mape : \", mape_p)'''\n",
        "\n",
        "    end_time = time.time()\n",
        "    execution_time = end_time - start_time\n",
        "    print(\"Execution Time:\", execution_time, \"seconds\")\n"
      ],
      "metadata": {
        "id": "AZ-x7G7B40Xq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b2181efa-7859-4acb-96d2-751ce7dd3cc0"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1 Physical GPUs, 1 Logical GPUs\n",
            "Enter the target column: Close\n",
            "Enter the columns (comma-separated) you want to add regression for: High,Low,Open,Volume,Adj Close\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:prophet:Disabling daily seasonality. Run prophet with daily_seasonality=True to override this.\n",
            "DEBUG:cmdstanpy:input tempfile: /tmp/tmpmzd92h2_/80lg0gnb.json\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training LSTM-NDT : complete\n",
            "**********************Column name for regression:  High\n",
            "**********************Column name for regression:  Low\n",
            "**********************Column name for regression:  Open\n",
            "**********************Column name for regression:  Volume\n",
            "**********************Column name for regression:  Adj Close\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "DEBUG:cmdstanpy:input tempfile: /tmp/tmpmzd92h2_/ygybytld.json\n",
            "DEBUG:cmdstanpy:idx 0\n",
            "DEBUG:cmdstanpy:running CmdStan, num_threads: None\n",
            "DEBUG:cmdstanpy:CmdStan args: ['/usr/local/lib/python3.10/dist-packages/prophet/stan_model/prophet_model.bin', 'random', 'seed=18288', 'data', 'file=/tmp/tmpmzd92h2_/80lg0gnb.json', 'init=/tmp/tmpmzd92h2_/ygybytld.json', 'output', 'file=/tmp/tmpmzd92h2_/prophet_model5pq15m7b/prophet_model-20230617121529.csv', 'method=optimize', 'algorithm=lbfgs', 'iter=10000']\n",
            "12:15:29 - cmdstanpy - INFO - Chain [1] start processing\n",
            "INFO:cmdstanpy:Chain [1] start processing\n",
            "2023-06-17 12:15:29,869 - __main__ - INFO - epoch (0/100), Loss: 0.66847930, ACU_loss: 4.67935508\n",
            "2023-06-17 12:15:29,869 - __main__ - INFO - epoch (0/100), Loss: 0.66847930, ACU_loss: 4.67935508\n",
            "INFO:__main__:epoch (0/100), Loss: 0.66847930, ACU_loss: 4.67935508\n",
            "2023-06-17 12:15:29,961 - __main__ - INFO - epoch (1/100), Loss: 0.67060336, ACU_loss: 4.69422352\n",
            "2023-06-17 12:15:29,961 - __main__ - INFO - epoch (1/100), Loss: 0.67060336, ACU_loss: 4.69422352\n",
            "INFO:__main__:epoch (1/100), Loss: 0.67060336, ACU_loss: 4.69422352\n",
            "2023-06-17 12:15:30,064 - __main__ - INFO - epoch (2/100), Loss: 0.65286250, ACU_loss: 4.57003748\n",
            "2023-06-17 12:15:30,064 - __main__ - INFO - epoch (2/100), Loss: 0.65286250, ACU_loss: 4.57003748\n",
            "INFO:__main__:epoch (2/100), Loss: 0.65286250, ACU_loss: 4.57003748\n",
            "12:15:30 - cmdstanpy - INFO - Chain [1] done processing\n",
            "INFO:cmdstanpy:Chain [1] done processing\n",
            "12:15:30 - cmdstanpy - ERROR - Chain [1] error: error during processing Communication error on send\n",
            "ERROR:cmdstanpy:Chain [1] error: error during processing Communication error on send\n",
            "WARNING:prophet.models:Optimization terminated abnormally. Falling back to Newton.\n",
            "DEBUG:cmdstanpy:input tempfile: /tmp/tmpmzd92h2_/a6k_jeu3.json\n",
            "DEBUG:cmdstanpy:input tempfile: /tmp/tmpmzd92h2_/r3p81_uy.json\n",
            "DEBUG:cmdstanpy:idx 0\n",
            "DEBUG:cmdstanpy:running CmdStan, num_threads: None\n",
            "DEBUG:cmdstanpy:CmdStan args: ['/usr/local/lib/python3.10/dist-packages/prophet/stan_model/prophet_model.bin', 'random', 'seed=9928', 'data', 'file=/tmp/tmpmzd92h2_/a6k_jeu3.json', 'init=/tmp/tmpmzd92h2_/r3p81_uy.json', 'output', 'file=/tmp/tmpmzd92h2_/prophet_modele0g8v_wu/prophet_model-20230617121530.csv', 'method=optimize', 'algorithm=newton', 'iter=10000']\n",
            "12:15:30 - cmdstanpy - INFO - Chain [1] start processing\n",
            "INFO:cmdstanpy:Chain [1] start processing\n",
            "2023-06-17 12:15:30,495 - __main__ - INFO - epoch (3/100), Loss: 0.65572067, ACU_loss: 4.59004468\n",
            "2023-06-17 12:15:30,495 - __main__ - INFO - epoch (3/100), Loss: 0.65572067, ACU_loss: 4.59004468\n",
            "INFO:__main__:epoch (3/100), Loss: 0.65572067, ACU_loss: 4.59004468\n",
            "2023-06-17 12:15:30,577 - __main__ - INFO - epoch (4/100), Loss: 0.64901739, ACU_loss: 4.54312176\n",
            "2023-06-17 12:15:30,577 - __main__ - INFO - epoch (4/100), Loss: 0.64901739, ACU_loss: 4.54312176\n",
            "INFO:__main__:epoch (4/100), Loss: 0.64901739, ACU_loss: 4.54312176\n",
            "2023-06-17 12:15:30,701 - __main__ - INFO - epoch (5/100), Loss: 0.65694079, ACU_loss: 4.59858555\n",
            "2023-06-17 12:15:30,701 - __main__ - INFO - epoch (5/100), Loss: 0.65694079, ACU_loss: 4.59858555\n",
            "INFO:__main__:epoch (5/100), Loss: 0.65694079, ACU_loss: 4.59858555\n",
            "2023-06-17 12:15:30,803 - __main__ - INFO - epoch (6/100), Loss: 0.65041327, ACU_loss: 4.55289286\n",
            "2023-06-17 12:15:30,803 - __main__ - INFO - epoch (6/100), Loss: 0.65041327, ACU_loss: 4.55289286\n",
            "INFO:__main__:epoch (6/100), Loss: 0.65041327, ACU_loss: 4.55289286\n",
            "2023-06-17 12:15:30,907 - __main__ - INFO - epoch (7/100), Loss: 0.66201227, ACU_loss: 4.63408589\n",
            "2023-06-17 12:15:30,907 - __main__ - INFO - epoch (7/100), Loss: 0.66201227, ACU_loss: 4.63408589\n",
            "INFO:__main__:epoch (7/100), Loss: 0.66201227, ACU_loss: 4.63408589\n",
            "2023-06-17 12:15:31,008 - __main__ - INFO - epoch (8/100), Loss: 0.64420908, ACU_loss: 4.50946355\n",
            "2023-06-17 12:15:31,008 - __main__ - INFO - epoch (8/100), Loss: 0.64420908, ACU_loss: 4.50946355\n",
            "INFO:__main__:epoch (8/100), Loss: 0.64420908, ACU_loss: 4.50946355\n",
            "2023-06-17 12:15:31,104 - __main__ - INFO - epoch (9/100), Loss: 0.64769570, ACU_loss: 4.53386992\n",
            "2023-06-17 12:15:31,104 - __main__ - INFO - epoch (9/100), Loss: 0.64769570, ACU_loss: 4.53386992\n",
            "INFO:__main__:epoch (9/100), Loss: 0.64769570, ACU_loss: 4.53386992\n",
            "2023-06-17 12:15:31,198 - __main__ - INFO - epoch (10/100), Loss: 0.64609326, ACU_loss: 4.52265280\n",
            "2023-06-17 12:15:31,198 - __main__ - INFO - epoch (10/100), Loss: 0.64609326, ACU_loss: 4.52265280\n",
            "INFO:__main__:epoch (10/100), Loss: 0.64609326, ACU_loss: 4.52265280\n",
            "2023-06-17 12:15:31,300 - __main__ - INFO - epoch (11/100), Loss: 0.65117430, ACU_loss: 4.55822009\n",
            "2023-06-17 12:15:31,300 - __main__ - INFO - epoch (11/100), Loss: 0.65117430, ACU_loss: 4.55822009\n",
            "INFO:__main__:epoch (11/100), Loss: 0.65117430, ACU_loss: 4.55822009\n",
            "2023-06-17 12:15:31,396 - __main__ - INFO - epoch (12/100), Loss: 0.65223790, ACU_loss: 4.56566530\n",
            "2023-06-17 12:15:31,396 - __main__ - INFO - epoch (12/100), Loss: 0.65223790, ACU_loss: 4.56566530\n",
            "INFO:__main__:epoch (12/100), Loss: 0.65223790, ACU_loss: 4.56566530\n",
            "2023-06-17 12:15:31,499 - __main__ - INFO - epoch (13/100), Loss: 0.64228904, ACU_loss: 4.49602330\n",
            "2023-06-17 12:15:31,499 - __main__ - INFO - epoch (13/100), Loss: 0.64228904, ACU_loss: 4.49602330\n",
            "INFO:__main__:epoch (13/100), Loss: 0.64228904, ACU_loss: 4.49602330\n",
            "2023-06-17 12:15:31,617 - __main__ - INFO - epoch (14/100), Loss: 0.64683462, ACU_loss: 4.52784234\n",
            "2023-06-17 12:15:31,617 - __main__ - INFO - epoch (14/100), Loss: 0.64683462, ACU_loss: 4.52784234\n",
            "INFO:__main__:epoch (14/100), Loss: 0.64683462, ACU_loss: 4.52784234\n",
            "2023-06-17 12:15:31,725 - __main__ - INFO - epoch (15/100), Loss: 0.64653847, ACU_loss: 4.52576929\n",
            "2023-06-17 12:15:31,725 - __main__ - INFO - epoch (15/100), Loss: 0.64653847, ACU_loss: 4.52576929\n",
            "INFO:__main__:epoch (15/100), Loss: 0.64653847, ACU_loss: 4.52576929\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training GDN : complete\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "12:16:11 - cmdstanpy - INFO - Chain [1] done processing\n",
            "INFO:cmdstanpy:Chain [1] done processing\n",
            "INFO:prophet:Disabling daily seasonality. Run prophet with daily_seasonality=True to override this.\n",
            "DEBUG:cmdstanpy:input tempfile: /tmp/tmpmzd92h2_/38k1p710.json\n",
            "DEBUG:cmdstanpy:input tempfile: /tmp/tmpmzd92h2_/jp8sh8fu.json\n",
            "DEBUG:cmdstanpy:idx 0\n",
            "DEBUG:cmdstanpy:running CmdStan, num_threads: None\n",
            "DEBUG:cmdstanpy:CmdStan args: ['/usr/local/lib/python3.10/dist-packages/prophet/stan_model/prophet_model.bin', 'random', 'seed=91853', 'data', 'file=/tmp/tmpmzd92h2_/38k1p710.json', 'init=/tmp/tmpmzd92h2_/jp8sh8fu.json', 'output', 'file=/tmp/tmpmzd92h2_/prophet_model5xyd27bc/prophet_model-20230617121612.csv', 'method=optimize', 'algorithm=lbfgs', 'iter=10000']\n",
            "12:16:12 - cmdstanpy - INFO - Chain [1] start processing\n",
            "INFO:cmdstanpy:Chain [1] start processing\n",
            "12:16:15 - cmdstanpy - INFO - Chain [1] done processing\n",
            "INFO:cmdstanpy:Chain [1] done processing\n",
            "INFO:prophet:Disabling daily seasonality. Run prophet with daily_seasonality=True to override this.\n",
            "DEBUG:cmdstanpy:input tempfile: /tmp/tmpmzd92h2_/stvp6nht.json\n",
            "DEBUG:cmdstanpy:input tempfile: /tmp/tmpmzd92h2_/tq8v7dih.json\n",
            "DEBUG:cmdstanpy:idx 0\n",
            "DEBUG:cmdstanpy:running CmdStan, num_threads: None\n",
            "DEBUG:cmdstanpy:CmdStan args: ['/usr/local/lib/python3.10/dist-packages/prophet/stan_model/prophet_model.bin', 'random', 'seed=87870', 'data', 'file=/tmp/tmpmzd92h2_/stvp6nht.json', 'init=/tmp/tmpmzd92h2_/tq8v7dih.json', 'output', 'file=/tmp/tmpmzd92h2_/prophet_model0aobe_ll/prophet_model-20230617121617.csv', 'method=optimize', 'algorithm=lbfgs', 'iter=10000']\n",
            "12:16:17 - cmdstanpy - INFO - Chain [1] start processing\n",
            "INFO:cmdstanpy:Chain [1] start processing\n",
            "12:16:21 - cmdstanpy - INFO - Chain [1] done processing\n",
            "INFO:cmdstanpy:Chain [1] done processing\n",
            "INFO:prophet:Disabling daily seasonality. Run prophet with daily_seasonality=True to override this.\n",
            "DEBUG:cmdstanpy:input tempfile: /tmp/tmpmzd92h2_/ztfxz927.json\n",
            "DEBUG:cmdstanpy:input tempfile: /tmp/tmpmzd92h2_/k4_jwa2z.json\n",
            "DEBUG:cmdstanpy:idx 0\n",
            "DEBUG:cmdstanpy:running CmdStan, num_threads: None\n",
            "DEBUG:cmdstanpy:CmdStan args: ['/usr/local/lib/python3.10/dist-packages/prophet/stan_model/prophet_model.bin', 'random', 'seed=34768', 'data', 'file=/tmp/tmpmzd92h2_/ztfxz927.json', 'init=/tmp/tmpmzd92h2_/k4_jwa2z.json', 'output', 'file=/tmp/tmpmzd92h2_/prophet_modelekdk3vn0/prophet_model-20230617121622.csv', 'method=optimize', 'algorithm=lbfgs', 'iter=10000']\n",
            "12:16:22 - cmdstanpy - INFO - Chain [1] start processing\n",
            "INFO:cmdstanpy:Chain [1] start processing\n",
            "12:16:27 - cmdstanpy - INFO - Chain [1] done processing\n",
            "INFO:cmdstanpy:Chain [1] done processing\n",
            "INFO:prophet:Disabling daily seasonality. Run prophet with daily_seasonality=True to override this.\n",
            "DEBUG:cmdstanpy:input tempfile: /tmp/tmpmzd92h2_/gvyufda3.json\n",
            "DEBUG:cmdstanpy:input tempfile: /tmp/tmpmzd92h2_/exj6hyh5.json\n",
            "DEBUG:cmdstanpy:idx 0\n",
            "DEBUG:cmdstanpy:running CmdStan, num_threads: None\n",
            "DEBUG:cmdstanpy:CmdStan args: ['/usr/local/lib/python3.10/dist-packages/prophet/stan_model/prophet_model.bin', 'random', 'seed=43264', 'data', 'file=/tmp/tmpmzd92h2_/gvyufda3.json', 'init=/tmp/tmpmzd92h2_/exj6hyh5.json', 'output', 'file=/tmp/tmpmzd92h2_/prophet_modelzq_f95am/prophet_model-20230617121628.csv', 'method=optimize', 'algorithm=lbfgs', 'iter=10000']\n",
            "12:16:28 - cmdstanpy - INFO - Chain [1] start processing\n",
            "INFO:cmdstanpy:Chain [1] start processing\n",
            "12:16:31 - cmdstanpy - INFO - Chain [1] done processing\n",
            "INFO:cmdstanpy:Chain [1] done processing\n",
            "INFO:prophet:Disabling daily seasonality. Run prophet with daily_seasonality=True to override this.\n",
            "DEBUG:cmdstanpy:input tempfile: /tmp/tmpmzd92h2_/ase8gpks.json\n",
            "DEBUG:cmdstanpy:input tempfile: /tmp/tmpmzd92h2_/5_x2aese.json\n",
            "DEBUG:cmdstanpy:idx 0\n",
            "DEBUG:cmdstanpy:running CmdStan, num_threads: None\n",
            "DEBUG:cmdstanpy:CmdStan args: ['/usr/local/lib/python3.10/dist-packages/prophet/stan_model/prophet_model.bin', 'random', 'seed=18248', 'data', 'file=/tmp/tmpmzd92h2_/ase8gpks.json', 'init=/tmp/tmpmzd92h2_/5_x2aese.json', 'output', 'file=/tmp/tmpmzd92h2_/prophet_model5ae1pr53/prophet_model-20230617121633.csv', 'method=optimize', 'algorithm=lbfgs', 'iter=10000']\n",
            "12:16:33 - cmdstanpy - INFO - Chain [1] start processing\n",
            "INFO:cmdstanpy:Chain [1] start processing\n",
            "12:16:37 - cmdstanpy - INFO - Chain [1] done processing\n",
            "INFO:cmdstanpy:Chain [1] done processing\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "True\n",
            "True\n",
            "Training LSTM-NDT : complete\n",
            "Execution Time: 70.46269512176514 seconds\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "_, test_result = test(gdn_model, test_dataloader, full_edges)\n",
        "_, val_result = test(gdn_model, val_dataloader, full_edges)\n",
        "log_final_results(test_result, val_result, config_gdn['report'])\n",
        "\n",
        "print (\"mae : \", mae_gdn)\n",
        "print (\"mse : \", mse_gdn)\n",
        "print (\"mape : \", mape_gdn)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nMe4JhbnQhHe",
        "outputId": "781246c1-dbff-40c9-e9bb-cfe7b7c3500c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-----------------------------------\n",
            "Final Results :\n",
            "-----------------------------------\n",
            "Best model : GDN\n",
            "Number of point anomalies: 158\n",
            "number of collective anomalies 8\n",
            "Grouped anomaly indices:\n",
            "[3, 9]\n",
            "[16, 18]\n",
            "[40, 63]\n",
            "[66, 68]\n",
            "[83, 96]\n",
            "[111, 150]\n",
            "[162, 163]\n",
            "[171, 235]\n",
            "mae :  0.6684792978422982\n",
            "mse :  0.810350775718689\n",
            "mape :  104.41462707519531\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "lstmndt_model.test()\n",
        "print (\"mae : \", mae_lstmndt)\n",
        "print (\"mse : \", mse_lstmndt)\n",
        "print (\"mape : \", mape_lstmndt)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fv6DolBkRuo_",
        "outputId": "92337f6c-ada1-4db4-8bc8-ca330c3d66a2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-----------------------------------\n",
            "Final Results :\n",
            "-----------------------------------\n",
            "Best model : LSTM-NDT\n",
            "Total channel sets evaluated: 6\n",
            "Avg normalized prediction error: 0.21392233647334583\n",
            "Total number of values evaluated: 5580\n",
            "Total collective anomalies found: 1\n",
            "Total number of point anomalies: 218\n",
            "Anomaly sequences start/end indices:\n",
            "[(400, 617)]\n",
            "mae :  0.5493184924125671\n",
            "mse :  0.5416778922080994\n",
            "mape :  1311.21826171875\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prophet_model.log_final_results(forecasting_for_oneyear)\n",
        "print (\"mae : \", mae_p)\n",
        "print (\"mse : \", mse_p)\n",
        "print (\"mape : \", mape_p)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "USG1YN9rS4FM",
        "outputId": "5693052a-9f98-4e94-ac17-465f2a54fe01"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-----------------------------------\n",
            "Final Results :\n",
            "-----------------------------------\n",
            "Best model : PROPHET\n",
            "Total number of point anomalies:  746\n",
            "Total number of collective anomalies :  566\n",
            "Indices of anomalies: \n",
            "[20, 168, 210, 231, 11, 13, 19, 21, 104, 146]\n",
            "[211, 232, 31, 36, 22, 83, 170, 212, 233, 55]\n",
            "[61, 1, 23, 148, [70, 71], [74, 75], [77, 78], 82, 149, 191]\n",
            "[[96, 97], [99, 100], 102, 113, [119, 120], [123, 124], 65, 193, 235, 136]\n",
            "[139, 141, [144, 145], 194, 236, 162, [166, 167], 26, 67, 151]\n",
            "[173, 237, 177, 183, [186, 187], 88, 198, [200, 201], 203, 6]\n",
            "[[221, 222], 225, 228, 230, 7, 48, 90, 154, 219, 239]\n",
            "[[241, 243], [245, 246], [248, 249], 335, 464, [264, 267], [269, 272], 274, 465, 282]\n",
            "[[285, 291], 316, 379, 401, 443, 302, [306, 312], 254, 276, 317]\n",
            "[402, 444, 324, 329, 331, [333, 334], 255, 380, 423, 445]\n",
            "[348, [350, 351], [354, 355], 296, 381, 424, 446, 467, 488, [367, 370]]\n",
            "[[373, 375], 378, 257, 277, 297, 361, 382, 403, 425, 489]\n",
            "[[386, 387], 389, 395, 398, 278, 298, 404, 426, [408, 410], 415]\n",
            "[419, 279, 341, 405, 447, 470, 430, 432, [434, 439], 441]\n",
            "[258, 300, 406, 471, 452, [454, 458], [462, 463], 281, 343, 384]\n",
            "[427, 449, 472, [474, 479], 481, [483, 484], 323, 364, 385, 450]\n",
            "[[492, 495], [497, 498], [501, 503], 545, 587, 609, [515, 516], 518, [521, 524], 546]\n",
            "[588, 630, 652, 690, 534, 536, [538, 543], 506, 568, 589]\n",
            "[653, 733, [553, 554], 556, [558, 562], [564, 566], 569, 590, 674, 692]\n",
            "[734, [576, 580], 582, [584, 585], 508, 528, 570, 611, 632, 693]\n",
            "[735, [597, 598], [600, 603], 607, 548, 571, 654, 676, 736, [617, 618]]\n",
            "[[622, 623], [625, 626], 629, 591, 613, 655, 638, [641, 642], 644, 647]\n",
            "[[649, 650], 509, 531, 550, 592, 659, 665, 672, 593, 634]\n",
            "[718, [679, 680], [682, 684], [687, 688], 511, 573, 594, 635, 658, 678]\n",
            "[738, 701, [706, 711], 615, 720, 723, 730, 732, 552, 575]\n",
            "[616, 637, 698, 719, [740, 742], 745, [749, 750], 774, 793, 813]\n",
            "[835, 877, 899, 764, [768, 769], [771, 773], 753, 814, 900, [786, 788]]\n",
            "[792, 815, 837, 921, 943, 806, [808, 809], 794, 944, 965]\n",
            "[986, 823, 826, 829, [831, 832], 834, 880, 901, 923, 987]\n",
            "[[844, 847], 849, 851, 854, 856, 777, 796, 838, 902, [865, 866]]\n",
            "[[869, 871], 873, [875, 876], 797, 903, 945, [886, 893], [895, 898], 779, [907, 909]]\n",
            "[911, 916, 758, 882, 905, 989, [933, 934], [936, 938], 883, 948]\n",
            "[953, [956, 957], 961, 780, 821, 863, 975, 978, 781, 885]\n",
            "[971, 996, 1001, 1087, 1015, 1017, [1020, 1021], 1034, 1040, 1045]\n",
            "[1057, [1059, 1060], 1065, 1152, 1079, 1028, 1111, 1153, 1106, 1127]\n",
            "[1070, 1155, 1137, [1144, 1145], 1163, 1093, 1189, 1073, 1051, 1136]\n",
            "[[1226, 1227], 1230, 1033, 1269, 1257, 1315, 1279, 1328, 1330, 1332]\n",
            "[1300, 1377, 1344, 1394, 1399, 1262, 1345, 1457, 1347, 1519]\n",
            "[1543, 1699, [1579, 1580], 1589, 1709, 1598, 1516, 1773, 1761, 1872]\n",
            "[1827, 1868, 1915, 2032, 2022, 2049, 2159, 2068, 2103, 2130]\n",
            "[2144, 2149, 2151, [2153, 2154], 2156, 2227, 2167, 2176, 2163, 2190]\n",
            "[2164, 2209, 2216, 2230, 2233, 2242, 2143, 2208, 2229, 2254]\n",
            "[2452, 2494, [2273, 2278], 2282, 2389, 2431, 2454, 2313, [2315, 2316], 2334]\n",
            "[2341, 2345, 2477, 2455, 2376, [2384, 2385], 2456, [2397, 2398], [2402, 2404], 2406]\n",
            "[2409, 2435, 2457, 2499, 2421, 2268, 2393, 2436, 2458, 2500]\n",
            "[2440, [2442, 2443], 2447, 2450, 2394, 2459, [2462, 2466], 2468, [2470, 2471], 2290]\n",
            "[2310, 2438, [2483, 2484], [2486, 2489], [2491, 2492], 2375, 2507, [2526, 2527], [2529, 2530], 2516]\n",
            "[[2553, 2554], [2565, 2566], 2568, 2570, 2575, 2557, 2591, 2611, 2617, 2631]\n",
            "[2694, 2718, 2544, 2823, 2846, 2849, 2853, 3059, 3038, [3209, 3210]]\n",
            "[3170, 3179, 3171, 3331, 3370, 3737, 3795, 3971, [4015, 4017], 4194]\n",
            "[[4186, 4189], 4204, 4222, 4226, 4228, 4200, 4244, 4291, 4313, 4299]\n",
            "[4278, 4491, 4342, 4394, 4398, 4281, 4302, 4283, 4306, 4519]\n",
            "[4741, 4677, 4719, 4605, 4594, 4623, 4680, 4535, 4597, 4724]\n",
            "[4733, 4748, 4640, 4661, 4796, 4970, 4816, [4818, 4819], 4804, 4903]\n",
            "[4808, 4990, [5028, 5029], 5068, 5222, 5034, 5075, 5035, 5333, [5335, 5336]]\n",
            "[5340, 5344, 5390, 5353, 5366, 5375, 5330, 5552, 5682, 5641]\n",
            "[5592, [5614, 5615], 5679, 5752, 5742, 5791]\n",
            "mae :  0.9758363630482838\n",
            "mse :  2.1713229798421825\n",
            "mape :  0.00545900457616959\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "6NsUYCzhS70Z"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}